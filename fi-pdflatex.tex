%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this work into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  digital, %% This option enables the default options for the
           %% digital version of a document. Replace with `printed`
           %% to enable the default options for the printed version
           %% of a document.
  table,   %% Causes the coloring of tables. Replace with `notable`
           %% to restore plain tables.
  lof,     %% Prints the List of Figures. Replace with `nolof` to
           %% hide the List of Figures.
  lot,     %% Prints the List of Tables. Replace with `nolot` to
           %% hide the List of Tables.
  %% More options are listed in the user guide at
  %% <http://mirrors.ctan.org/macros/latex/contrib/fithesis/guide/mu/fi.pdf>.
]{fithesis3}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the T2A font encoding
\usepackage[T1,T2A]{fontenc}  %% to use the Cyrillic fonts with Russian texts.
\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
  %english, german, russian, czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%
%%   \begin{otherlanguage}{german}  ... \end{otherlanguage}
%%   \begin{otherlanguage}{russian} ... \end{otherlanguage}
%%   \begin{otherlanguage}{czech}   ... \end{otherlanguage}
%%   \begin{otherlanguage}{slovak}  ... \end{otherlanguage}
%%
%% For non-Latin scripts, it may be necessary to load additional
%% fonts:
\usepackage{paratype}
\usepackage{textcomp} % to be able to use normal apostrophe with \textquotesingle
\def\textrussian#1{{\usefont{T2A}{PTSerif-TLF}{m}{rm}#1}}
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{
    date          = 2018/05/20,
    university    = mu,
    faculty       = fi,
    type          = mgr,
    author        = Jiří Mauritz,
    gender        = m,
    advisor       = Ing. Libor Kubečka, 
    title         = {Automatic Classification of Legal Documents},
    TeXtitle      = {Automatic Classification of Legal Documents},
    keywords      = {machine learning, legal documents, natural language processing},
    TeXkeywords   = {machine learning, legal documents, natural language processing},
    abstract      = {This is the abstract of my thesis, which can

                     span multiple paragraphs.},
    thanks        = {These are the acknowledgements for my thesis, which can

                     span multiple paragraphs.},
    bib           = example.bib,
}
\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.
%% These additional packages are used within the document:
\usepackage{paralist} %% Compact list environments
\usepackage{amsmath}  %% Mathematics
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}      %% Hyperlinks
\usepackage{markdown} %% Lightweight markup
\usepackage{listings} %% Source code highlighting
\lstset{
  basicstyle      = \ttfamily,%
  identifierstyle = \color{black},%
  keywordstyle    = \color{blueečnou práci zavádí studijní oddělení },%
  keywordstyle    = {[2]\color{cyan}},%
  keywordstyle    = {[3]\color{olive}},%
  stringstyle     = \color{teal},%
  commentstyle    = \itshape\color{magenta}}
\usepackage{floatrow} %% Putting captions above tables
\usepackage{slashbox} %% Big slash in tables
\usepackage{cleveref}[2012/02/15] %% Enables cross reference footnotes
\floatsetup[table]{capposition=top}
\usepackage{minted} % nice listings
%\usemintedstyle{fruity}
\usepackage{amsmath,amssymb} % math
\DeclareRobustCommand{\bbone}{\text{\usefont{U}{bbold}{m}{n}1}}
\DeclareMathOperator{\EX}{\mathbb{E}}% expected value

\begin{document}
\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

introduction

\chapter{Related work}
This chapter summarizes the previous work on the application of artificial intelligence in the legal domain. 
The first section describes a general role of artificial intelligence in law and how it evolved in time.
Researchers in this field focused mainly on decision-making systems to support work of the jury; however, this research focuses more on the information retrieval and knowledge discovery in the legal domain, also known as \textit{Electronic Discovery}, which brings a support for lawyers.
Therefore, the second section looks more closely at this subfield by introducing state-of-the-art legal retrieval systems and underlines common characteristics with the research presented here.
The last section discusses an issue of relevance in the legal context and related studies.

\section{Artificial Intelligence and Law}
The global publishing company \textit{Springer} introduced a journal \textit{Artificial Intelligence and Law} \footnote{\url{https://link.springer.com/journal/10506}} in 1992 which contains 426 published articles (as of March 2018) describing theoretical and empirical studies in artificial intelligence (AI) from the legal perspective.
Though the journal consists of articles concerned with AI as a new trend that needs to be bound by law, such as legal aspects of autonomous vehicles \cite{Prakken2017autonomous}, robot sex \cite{Frank2017robotSex} or responsibility of autonomous machines \cite{Brozek2017AIresponsibility}, it also contains researches of new innovative AI systems that perform automatic reasoning and knowledge extraction in the legal domain.
The collection of AI-law related articles provides enough information to create a brief overview of how this field has evolved over time.

The first legal search engines enabled retrieving results by first-order logic rules and fixed pieces of text \cite{Turtle1995legalRetrieval}.
In the early 1990s, the concept of full-text search has become widely used and users could enjoy a retrieval by a plain natural language \cite{Turtle1995legalRetrieval}.
Shortly after, researchers started to focus on AI when creating a new legal retrieval system, such as legal text management system \textit{Flexlaw} (\citeyear{Smith1995flexlaw}) \cite{Smith1995flexlaw}, integrated decision support system \textit{DataLex WorkStations} (\citeyear{Greenleaf1995datalex}) \cite{Greenleaf1995datalex}, or system for heuristic retrieval of legal arguments \textit{BankXX} (\citeyear{Rissland1996bankxx}) \cite{Rissland1996bankxx}.
As the machine learning was still quite nascent, the mentioned systems are a fusion of expert and AI systems with expert knowledge as the essential part of the system.
A sign of such fusion is an attempt to formalize the \textit{legal argument} (\cite{Freeman1996argument}, \cite{Bench-Capon1997argument} and most recently \cite{ashley2013information}) as an element between the \textit{rules} (expert systems) and \textit{models} (AI systems).

At the turn of the century, an attempt to conceptualize the legal domain for the design of legal knowledge systems brought a number of ontologies, such as McCarty's language for legal discourse, Stamper's norma formalism, or Valente's functional ontology \cite{Visser1998ontologies}.
The ontologies are categorizing the knowledge and breaking it into predicate relations, which express properties of objects, rules in the form of implications and second-order expressions adding more explanatory power.
Unfortunately, they are not very widely applied, except for the Valente's Functional Ontology for Law (FOLaw) \cite{VanEngers2008folaw}.
The conclusion of the research comparing four main ontologies is
\begin{quote}
``None  of  the  ontologies  seems  to  have  adequate  provisions  to  specify  legal procedures.'' --- \citeauthor{Visser1998ontologies} \cite{Visser1998ontologies}
\end{quote}

Consequently, the formalistic implementations of the syllogistic model in legal domain were criticized (\cite{Shapira1999fuzzy}, \cite{Philipps1999nn}).
Researchers realized that the deductions starting at axioms and following prescribed rules are limiting and introduce imprecise standards in the legal reasoning.
One of the possible solutions is to use relaxed formalisms, such as fuzzy logic \cite{Shapira1999fuzzy} or neural networks \cite{Philipps1999nn}.
Philipps et al. suggest that the usual perspective of rules as a set of linguistic entities (sentences, words, etc.) is not general and abstract enough to express the knowledge \cite{Philipps1999nn}.
They compare the lawyer's work to the learning and producing results of the neural network.
Examples of real-world applications following this scheme are the neural approach to legal reasoning system for family law in Australia \cite{Stranieri1999nn}, modeling the French Council of State decisions in NEUROLEX \cite{Bourcier1999nn}, or a decision  support  system   in the field of insurance using a neural network \cite{Borgulya1999nn}.

% odstavec o 2000 - 2010 - jak pribyvalo dat -> data mining, lepsi modely
Despite all the effort, lawyers were rather skeptical about AI helping in the legal domain, as can be seen in the article \citetitle{oskamp2002ainotsomuch} by \citeauthor{oskamp2002ainotsomuch} (\citeyear{oskamp2002ainotsomuch}) \cite{oskamp2002ainotsomuch}.
The main problem seemed to be the lack of legal data, which could not have been applied to train and evaluate the AI models.
This, however, changed during the first decade of the new century with the rise of the Internet.
New collections of documents and structured databases enabled growth of a data exploration techniques:
\textit{data mining} was applied to find implicit information from multiple heterogeneous sources (\citetitle{Dahbur2003crimes} \cite{Dahbur2003crimes}), \textit{heuristic search} method was used in case-based reasoning system AGATHA \cite{Chorley2005agatha}, and \textit{text summarization} was applied in extracting the important parts of law from the XML corpus of judgments of the UK House of Lords \cite{Hachey2006summarization}.

% odstavec o 2010 - present - jak se zacly nastroje specializovat
During the last few years, the automated tools helping in legal realm have become more specialized, yet properly tuned.
These include automated tool for finding patents related to a particular topic \cite{Abood2018patent}, detection of tax evasion by networks of transactions with their corresponding audit observables \cite{Hemberg2016tax}, and finding and modeling contradictions in judicial statements \cite{Carey2013contradiction} \cite{marneffe2008contradiction}.

% bayesian networks
Some of the more theoretical researches has adopted \textit{Bayesian networks} for modeling legal arguments and eventually evidence (\cite{Fenton2012bayes} and \cite{Vlek2016bayes}).
The \textit{Bayesian network} is a directed acyclic graph with probability tables for each node \cite{Vlek2016bayes}.
\begin{figure}[H]
\caption{Simple example of Bayesian Network.}
\label{fig:bayesianNetwork}
\includegraphics[width=0.9\textwidth]{img/Bayesian_Network}
\end{figure}
Each node holds a predicate, which is true with certain probability depending on the value of incoming nodes.
For example, a table of the node $C$ in the figure \ref{fig:bayesianNetwork} would describe conditional probabilities of the events $P(C | A, B)$ for all possible combinations of values A and B \cite{Vlek2016bayes}.
The \textit{Bayesian Network} enables to visualize the legal arguments and is able to work with uncertainty.
The probabilities of unlikely events are especially of great importance since judges need to distinguish between coincidence and purpose.

\section{Electronic Discovery}
Electronic Discovery, also known as E-discovery or Electronic Data Discovery (EDD), is a process of searching, gathering, cleansing, and organizing of legal documents in order to find evidence in a civil or criminal legal case \cite{Conrad2010ediscovery}.
The field of Information Retrieval (IR) is an essential part of the E-Discovery, as it provides efficient full-text search among the documents.
\citeauthor{Conrad2010ediscovery} mentions how the IR can be supplemented by AI techniques in his work \citetitle{Conrad2010ediscovery} \cite{Conrad2010ediscovery}:
\begin{enumerate}
\item Intelligent relevance feedback
\item E-mail management
\item Social network analysis
\item Data mining and machine learning techniques
\item Anticipatory E-Discovery
\end{enumerate}
The E-mail and Social network have become naturally a source of legal arguments with increasing amount of information stored in such media.
Anticipatory E-Discovery is a new practice of corporates to prepare for a possible legal hold on some of their products or intellectual property.

The fourth category includes mainly the clustering and categorization of documents to organize them into separate groups.
A research under \textit{Xerox Research Center Europe} introduced a machine learning classifier \textit{CategoriX}, which works with a fixed set of document categories and predicts document's category with probabilistic latent semantic analysis (PLSA) \cite{barnett2009classification}.
The mentioned technique is usually applied in the topic modeling of documents and is briefly explained in the section \ref{topic_modeling} as it is one of the working attributes of the system proposed here.
Research of Noortwijk et al. presents the \textit{Copac} system, which categorizes whole legal cases instead of documents.
Moreover, they relaxed the set of categories and enable users to define their own by selecting examples of positive and negative samples \cite{van2006ranking}.

The main goal of this research is the relevance prediction.
However, the categorization of documents is also part of this research, because the preparation for the relevance prediction is labeling documents as case-related or law-related (\ref{sec:law-case-classifier}).
The paper \citetitle{thompson2001automatic} compares three algorithms for legal document classification: k-NN, decision tree algorithm, and a machine learning rule induction algorithm Ripper \cite{thompson2001automatic}.
They conclude the decision tree and Ripper algorithms outperform the k-NN with Ripper being best by recall measure.
For the purpose of the categorization discussed in this research, a decision tree was applied for its satisfactory performance, simplicity, and transparentness.
Moreover, the ensemble of decision trees -- Random Forest -- helped us to boost the performance.

The first enhancement of IR -- Intelligent relevance feedback -- best explains the workflow of the system proposed here.
By feedback, \citeauthor{Conrad2010ediscovery} means consultation of the suggested relevance by the system to the user, which might lead to recomputation of the relevances \cite{Conrad2010ediscovery}.
This iterative process has proven to increase the accuracy of the predictions \cite{Conrad2010ediscovery}.
On the other hand, Zhao et al. show that more than one round of the feedback selection process is not helpful \cite{zhao2009feedback}.
The proposed system provides a possibility of immediate feedback through relevance settings of individual documents.
Relevances within the scope of the particular legal case are updated after each feedback.

To compare the existing systems with the proposed, it is worth noting that most of them implement a slightly different use case.
Their goal is to pick relevant documents out of a massive storage of legal texts, in which documents might relate to multiple cases, whereas here the goal is to pick relevant documents out of a relatively small collection that is uploaded to the system by the lawyer.
The proposed system is not allowed to share documents among cases nor look for information elsewhere than what is selected by the lawyer.
This task is somewhat more difficult since the prediction model starts to learn almost from the beginning with each new case.
Most of the similar researches focus either on the case retrieval or document retrieval from a global storage, hence the use case of the proposed system seems to be too unusual to be included in any recent research.

% Papers predicting outcomes of new cases based on previous
% 1. Automatically classifying case texts and predicting outcomes

\section{Measuring relevance in legal domain}
Many studies are concerned with the concept of relevance in the legal domain, formalization of its subjective nature and its measuring and evaluation (\cite{vanOpijnen2017relevance}, \cite{Hogan2010sensemaking}, \cite{Ashley2010analysis}).
The following discussion takes into consideration a human ability to decide the relevance.

% paper: Document categorization in legal electronic discovery: computer classification vs. manual review.
When evaluating relevance decided by an automated system, the results should be always compared to the efficiency of humans.
Results in IR and E-Discovery are hard to evaluate since there is no ground truth.
Even among the humans annotators, the 100\% agreement is simply not possible -- the relevance is too subjective factor.
Roitblat et al. conduct an experiment that compares efficiency of computer classification and manual review \cite{roitblat2010comparison}.
They pick 5000 documents from a collection of 1.6 mil documents (9.8\% relevant) annotated by attorneys in the past for other purposes than research.
Consequently, a new group of attorneys divided into two teams A and B were reviewing the documents.
At the same time, two anonymous E-Discovery systems referred to as C and D were applied to classify all collection of 1.6 mil documents.
Precision of the manual reviews (percentage of correctly relevant from all classified as relevant) was 20\% and 18\% whereas the systems achieved precisions of 27\% and 29\%.
Recall of the manual reviews (percentage of correctly relevant from all relevant) was 49\% and 54\% whereas the systems achieved recalls of 46\% and 53\% \cite{roitblat2010comparison}.
Note that the the recalls are quite comparable, whereas the precision is significantly higher for the classification systems.
This research showed that the time invested in development of such systems is worthwhile.
The present research will refer to these results in the evaluation chapter (\ref{chp:evaluation}) to compare the precision and recall to both the human efficiency and state-of-the-art legal classification systems.

% Relevance - with higher precision, lower recall and vice versa, always better high recall
% paper: Automation of legal sensemaking in e-discovery
In ordinary IR, the precision is usually more significant measure than recall, since the user is often satisfied with the first few most relevant results.
In the legal domain, both high precision and high recall are required, sometimes the recall is considered the dominant measure as missing a relevant information could be fatal \cite{Hogan2010sensemaking}.
Hogan et al. divide the E-Discovery systems to two groups according to the method for balancing precision/recall.
First group is the CAHA systems (Computer Assisted Human Assessment), where humans assess and filter the result of the IR system \cite{Hogan2010sensemaking}.
The IR system returns a result with low precision but high recall and human selection rises the precision.
The other systems are HACA (Human Aided Computer Assessment), in which a human provides an annotated samples of documents to improve models of the IR system \cite{Hogan2010sensemaking}.
Here, the HACA system is a fully automatic system with high precision and high recall.
This research presents a system that falls into the latter group.

% paper: On the concept of relevance in legal information retrieval
Opijnen and Santos in their study \citetitle{vanOpijnen2017relevance} breaks down the relevance into six concepts, which are necessary to ensure relevant result \cite{vanOpijnen2017relevance}.
Whereas they understand the relevance in context of the IR, Hogat et al. in their study \citetitle{brassil2009usermodeling} relates the relevance to a supervised classification system, which is closer to the system presented here \cite{brassil2009usermodeling}.


As the definition of relevance is entirely derived from the user's requirements, Hogan et al. present a User Modeling (UM) approach to achieve high performance in search tasks \cite{brassil2009usermodeling}.
The four main concepts of UM are:
\begin{enumerate}
\item \textit{Use Case} -- determines the goals of the user and their preference towards precision and recall.
\item \textit{Scope} -- defines the relative amount of concepts that are understood by the user as relevant. The scope defines a threshold that tell which concepts are \textit{core} and which are \textit{peripheral}.
\item \textit{Nuance} -- is a measure determining how relevant are concepts within a document, considering lexical relationships.
\item \textit{Linguistic Variability} -- determines the variability of expressions of the concept. The expressions can be lexical or syntactic.
\end{enumerate}

% paper: Emerging AI & Law approaches to automating analysis and retrieval of electronically stored information in discovery proceedings

\chapter{Background: Legato System}
\chapter{Data Characteristics}
\label{chp:data_characteristics}
One of the most important preconditions for building an efficient model is to understand the input data characteristics.
People developed numerous models based on text classification that are suppose to classify text of any domain, however, at the expense of the efficiency.
The specialized models always have the potential to outperform the general one.
The real contribution of this work is the understanding of characteristics of the legal domain data and its application on the process of building a text classification model.
This chapter will introduce our working dataset, its statistics and characteristics.

\section{Dataset introduction}
The end users of the Legato system are mostly the U.S. lawyers, which influences our choice of the dataset.
The U.S. legal system is very specific with its own laws, constitution, trial process and most importantly for us -- terminology.
Also, we have to take into consideration specifics in the form of the documents: medical records, police records, affidavits, emergency call transcripts etc.
We do not claim that our application will not work for other than U.S. legal documents, however, the model might manage to recognize the structure and terminology of the U.S. documents more accurately.

Unfortunately, we have not acquired enough training data from the users of Legato, mainly because the legal cases are ongoing and the documents are too confidential to be included in a research.
For the same reason, it is hard to find an authentic public trial documents on the Internet.
We have found a solution thanks to an \textit{American Mock Trial Association} \footnote{http://www.collegemocktrial.org/} and \textit{Classroom Law Project} \footnote{\url{http://www.classroomlaw.org}}.
Both associations create legal cases for educational purposes, which then serve as a material for the mock trial competitions.
All their legal cases are fictional, though some of them were inspired by real-world situations and issues.
Still, any resemblance to actual persons and events is purely coincidental.

All 20 cases used as a training set are downloaded from two sites: Street Law, inc. \footnote{\url{http://streetlaw.org}} and Classroom Law Project \footnote{\url{http://www.classroomlaw.org}}.
Not to be biased with one legal area, we included cases of diverse legal fields:
\begin{itemize}
\itemsep0em
\item[2x] \textbf{School environment} \\ educational malpractice or indifference
\item[1x] \textbf{Bullying} \\ intentional infliction of emotional distress
\item[2x] \textbf{Criminal law} \\ assault, battery or murder
\item[3x] \textbf{Inmate partner violence} \\ domestic violence by a current or former partner
\item[3x] \textbf{First Amendment} \\ breach of freedom of speech, religion, press etc.
\item[4x] \textbf{Child neglect} \\ child injury or death caused by parents, baby-sitter or teacher
\item[2x] \textbf{Sex crime} \\ sexual harassment and disease transmissions
\item[3x] \textbf{Tort law } \\ fault leading to suffer or harm, without breaching any contract
\end{itemize}
All mock cases have a similar structure: there are approx. 5-8 testimonies of the involved people, called affidavits, and the rest of the documents are emails, medical records, police reports, emergency call transcripts, description of the past similar cases etc.
The affidavits are up to 10 pages long, usually describing the issue in depth, whereas the other documents are usually one or two pages long.
We excluded all extra information specific to the mock trials, explanations of the competitions and summaries: we do not expect these to be typical part of a case.
Some extra headers and footers that contain metadata were also removed not to confuse the model training.

During the model building, we need to take into account that the real cases might be more or less different from the mock cases.
For instance, we do not expect the affidavits to appear in the real cases as much as in the mock cases.
Also, we expect more emails and papers of unknown structure in real cases.
Generally, we found two major shifts: 
\begin{enumerate}
\item real cases will most probably contain shorter documents
\item real cases will contain many more unrelated documents
\end{enumerate}
We present solutions for both issues:
\begin{enumerate}
\item 
The length of the document should be an insignificant parameter when determining the documents relevancy.
Therefore, we decided to focus on sentences rather than on a document as a whole.
In other words, we determine relevance of the document by exploring the relevances of the individual sentences.
As a result, the model is slightly simplified, however, we do not completely ignore the cross-sentence information, as we will see in the further chapters.
Thanks to the sentence-based model, we should be able to eliminate an influence of the document length and judge the relevance without bias.

\item
The second issue is caused by the lack of motivation to add unrelated documents during the creation of the mock cases.
Still, we can find some documents that are related to the case but are not relevant, e.g. resume of a defendant.
A straight forward solution is to add some irrelevant documents from other cases to simulate the balance in a real situation.

\end{enumerate}

\section{Annotation}
- num of annotators
- annotation of docs
- annotation of sentences

\section{Data statistics}
As mentioned earlier, we are working with 20 cases with total of 261 documents, which means 13.15 documents per case in average.
The documents together contain 234,798 words, thus average number of words per document is 900.
We counted 14,518 unique words which is approx. 1.18 \% of English vocabulary according to the \textit{Oxford English Dictionary}  \cite{oxfordDictionaries}.

\begin{figure}[h]
\centering
\caption{Most frequent words in the dataset with and without stop words.}
\label{fig:most_frequent}
\footnotesize
\begin{tabular}{|l|r||l|r|}\hline
\rowcolor{orange!50}
\multicolumn{2}{|c||}{\makebox[9em]{\textbf{All words}}} & \multicolumn{2}{|c|}{\textbf{Without stop words}} \\\hline\hline
\textbf{word} & \textbf{count} & \textbf{word} & \textbf{count} \\\hline\hline
the & 10825 & school & 855 \\\hline
to & 6632 & would & 683 \\\hline
I & 6274 & one & 566 \\\hline
and & 5912 & time & 497 \\\hline
of & 5479 & 1 & 476 \\\hline
a & 4896 & said & 451 \\\hline
in & 3603 & told & 420 \\\hline
that & 3499 & could & 409 \\\hline
was & 2846 & get & 391 \\\hline
for & 1942 & like & 386 \\\hline
\end{tabular}
\normalsize
\end{figure}

In the left part of the figure \ref{fig:most_frequent}, we present counts of the most frequent words in the documents.
It is not a surprise that all top ten words are common stop words of English.
The stop words are usually removed before any further processing of the text, however, they might carry valuable information too.
For instance, recognition of the author or formality level can be determined based on the stop words usage.
We can see that the usage of stop words is not different from the general English.

The right side of the table shows most frequent words after we removed the stop words.
Here, we can see clear influence of the cases from the school environment.
Next, the words "said" and "told" indicate that the documents often contain reporting of events and conversations.
The other words again fit more or less in the general English.

The word frequencies tell us that the vocabulary is not biased by any major trend.
Lets now look at the keywords of the whole collection and discover words that occur more frequently than usual.
The method used is explained in section \ref{algorithms_keywords} and we applied the same reference corpus (Reuters articles).
The following are the keywords: city, school, know, got, joey, just, police, princess, time, person, really, like, child.

- most frequent words for crime vs. first amendment
- most frequent words for relevant vs. unrelevant cases

\section{Types of documents}
- non-sense docs (pics)
- law related vs. case specific - loosely define
\section{Linguistics}


\chapter{NLP tool selection}
The amount of text in legal documents can vary largely.
Most of the documents we are working with stretch across one page, however, the average number of pages is six and we can find documents up to hundred pages long.
The typical approach of the text processing is to convert the text into a document-term matrix that contains number of term occurrences in each document.
The problem with this approach is that the long documents are represented by a vector, which \emph{information gain} is rather low.
High \emph{information gain} implies that the numerical representation captures the general idea, the purpose of the document, and characteristics of similarity/dissimilarity with other documents.

To address the issue, we decided to represent a document with specific features extracted by one or more NLP tools rather than plain words of the document.
We are looking for the most interesting pieces of information in the text, such as people, locations, dates, crime-related words or email addresses.
Nowadays, one can find numerous services providing extraction of such features.
We have chosen some of them and have collected results from testing on our mock legal case.
We compared results of the NLP services and a survey, in which participants labeled features from the text in a similar way as an NLP tool.

It is important to note that the field of NLP feature extraction is a well-studied topic and the tools produce very stable results, probably using rather similar algorithms (which are described in the following section). Even though we could dedicate part of this work to developing well-suited algorithms for NLP feature extraction, we decided to take advantage of the existing tools and put the focus on processing and application of the features to the high-quality data mining and machine learning algorithms. 

\section{NLP Algorithms}
First, let us introduce the features and well-known algorithms for their extraction from a text in general.
Note that the further mentioned tools might use modified or even different techniques for extraction of the features.

\subsection{Keywords}
\label{algorithms_keywords}
\emph{Keywords} are people, places, words, ideas, which are understood as important in the given context. 
In our case, the context is the document, hence we expect the keyword to reflect what the document is really about.
Then, the \textit{keyness} is a quality measure of the importance in the given text.
\cite[ch. 4]{scott2006textual}

We do not restrict the keyword to be one word because most of the tools are able to extract multi-word keywords, so-called \textit{key phrases}.
By combining words together, the phrases usually gain a new meaning that cannot be inferred from the individual words.
Therefore, if we assumed only one-word keywords, we would never find the keyness hidden in the phrases.

The process of identifying keywords is driven by two factors.
First, the more often a word occurs in the document, the more likely it is a keyword.
And second, the more often a word occurs generally in a speech, the less likely it is a keyword of any document.
\cite[ch. 4]{scott2006textual}
The second factor ensures that words we use very often in the speech, such as prepositions, conjunctions or the most common nouns, are not considered as keywords, even though their occurrence is frequent.

Common implementation of the idea is called the \textit{TF-IDF} algorithm \cite[ch. 6]{manning2008introduction}.
The weight of a term is defined by its \textit{term frequency} and \textit{inverse document frequency}.
The term frequency tf$_{t,d}$ is simply a number of occurrences of the term $t$ in the document $d$. The \textit{inverse document frequency} is defined as
$$
\text{idf}_t = \log{\frac{N}{\text{df}_t}}
$$
where $N$ is the total number of documents and df$_{t}$ is a \textit{document frequency}, which is equal to the number of documents containing term $t$.
It is important to take a logarithm of the expression because the \textit{inverted document frequency} can become very large with a large collection of documents and we need it to be comparable with the \textit{term frequency}, which is rather small.

The tf-idf weight is computed as follows \cite[ch. 6]{manning2008introduction}:
$$
\text{tf-idf}_{t, d} = \text{tf}_{t,d} \times \text{idf}_t
$$
The \textit{term frequency} ensures higher weight for more frequent terms in the document and \textit{inverse document frequency} ensures higher weight for more obscure words. Now, tf-idf$_{t, d}$ can be a good approximation of our keyness measure and is often used in the keyword extraction technique with the same same. The last step is to establish a threshold value that distinguishes keywords and non-keywords by comparing it to the tf-idf values of the terms.

\subsection{Named Entity Recognition}
To extract valuable information from the text, it is necessary to include \textit{named entities} (NEs).
They are entities that can be referred to with a proper name \cite[ch. 21]{jurafsky2014speech}.
In the speech, NEs are always noun phrases representing people, places, things, organizations, temporal or numerical expressions, events etc.

Lets pinpoint how important the named entities are in the context of legal cases.
Consider a typical use case of our application: a lawyer is searching through hundreds of emails with different senders and receivers.
The name of the only victim, place, and date of the crime are known.
If the lawyer is forced to manually search through the documents, they will filter the emails by occurrences of the aforementioned facts in the text.
All of the facts are named entities, which can be extracted from each document automatically and eventually lead the algorithm to increase the relevance of documents accordingly.
Thanks to the named entities, the algorithm performs the classification in the same manner as the lawyer would do, which is our goal.

The academic approach to find a \textit{named entity} is often based on the statistical sequence model \cite[ch. 21]{jurafsky2014speech}.
Entity type and boundaries are found with one pass over the text according to a set of rules.
The rules include \textit{Part-of-Speech Tagging} 
\footnote{Detailed information about POS Tagging can be found in chapter 10 of \citetitle{jurafsky2014speech} by \citeauthor{jurafsky2014speech} \cite{jurafsky2014speech}}
, which helps to find the noun phrases, and therefore, the boundaries.
Next, the \textit{word shapes} are examined, which are simple string representations that generalize a group of expressions.
For instance, replacing any letter for X and any digit for d, we can describe a unique word shape dd XX, which represents hours of the day written in US format, such as 12 am or 03 PM. 
Other rules include prefix matching or dictionary of predefined named entities.

In comparison to the academic approach, the commercial approach is based on the combination of updated dictionaries, rules and supervised machine learning \cite{chiticariu2013rule}.
In the word of Big Data, it has become easier to collect large dictionaries of given names, family names, organizations or geological places (gazetteers).
The named entity recognition is performed by multiple passes over the text data.
First, the high-precision but low-recall rules are applied, then more entities are added by substring search of the previous entities.
In the next phase, the entities are consulted with the dictionaries and only then, the statistical sequence model is applied considering all the features from the previous stages \cite[ch. 21]{jurafsky2014speech}.

Further in the thesis and the application, we will work with the following set of \textit{named entities}:
\begin{itemize}
\item Person
\item Organization
\item Location
\item Time
\item Email Address
\item Crime
\item Health Condition
\end{itemize}

\subsection{Relations}
Once we have extracted the \textit{named entities}, we can start looking for the relationships between them.
As soon as we recognize entities and their relations, the main semantic meaning of a sentence is usually fully extracted.
In the legal environment, we can illustrate the added value of a relation by a testimony, in which a witness tells about both a person and a location, however, we are interested in the document only if there is a relation between these two.

Lets see how the relations are represented.
Most of the \textit{relations extraction} algorithms are focused on binary relations only.
Multi-entity relations would be very expensive to extract.
We will borrow the representation from the Resource Description Framework (RDF) \cite{lassila1999resource} and define the relation as a triple
$$
\text{<subject, predicate, object>}
$$
where the \textit{subject} is an entity that performs an activity specified in the \textit{predicate} with respect to the \textit{object}.
In our context, the \textit{predicate} is the type of the relation; it can be, for example, "employed-by", "built", "son-of" or "cheated-on".

The following groups of algorithms can be applied to extract the relations: \textbf{hand-written patterns}, \textbf{supervised machine learning}, \textbf{semi-supervised}, and \textbf{unsupervised}.
We will introduce the first two
\footnote{For more detailed description, see \citetitle{jurafsky2014speech} \cite{jurafsky2014speech}.}.

\subsubsection{\textbf{Hand-written patterns}}
The work \citetitle{hearst1992automatic} by \citeauthor{hearst1992automatic} \cite{hearst1992automatic} describes the first idea of using patterns to extract relations.
The entity names are ignored and a relation is recognized by a set of patterns.
The patterns are based on the lexicology (POS tagging), the syntax (syntax tree) and the type of the entity (person, location, etc.).
For example, the pattern \cite[sec. 21.2]{jurafsky2014speech}
$$
NP_0\text{ such as }NP_1 \{, NP_2 ..., (\text{and|or})NP_i \}, i \geq 1
$$
is able to catch the relation hyponym for many occurrences, for instance, a sentence
$$
\text{... by any poison, such as Cyanide, Arsenic or Ricin ...}
$$
would generate relations 
\begin{itemize}
\item <poison, hyponym of, Cyanide>
\item <poison, hyponym of, Arsenic>
\item <poison, hyponym of, Ricin>
\end{itemize}
An example of a pattern using entity type \cite[sec. 21.2]{jurafsky2014speech}
$$
\text{\textbf{PERSON} (named|appointed|chose|...) \textbf{PERSON} Prep? \textbf{POSITION}}
$$
will determine a relation "named". 

The big disadvantage of the pattern-based recognition is that the patterns need to be written by hand, which takes a lot of work. On the other hand, the precision of such method is usually high since the patterns are directed by a human.

\subsubsection{\textbf{Supervised machine learning}}
The second approach is to annotate a corpus of relations and entities and consider it as a training set in the machine learning.
More precisely, features are extracted from the sentences and are served to the machine learning algorithm as an input.
With the relation types as the output labels, the algorithm is prepared to be trained automatically.
\cite[sec 21.2]{jurafsky2014speech}.

The features can consist of unigrams or bigrams of the words in the sentence, especially between the subject and object. Next, the named entity types, POS tags or features from the syntax tree can be included as well. Sometimes the number of words between the subject and object or stemmed version of the words is also be considered \cite[sec. 21.2]{jurafsky2014speech}.

The algorithm uses two models for the relations extraction.
The first decides whether there exists a relation between two given entities, and the second one recognizes its type. First, the algorithm finds all pairs of entities in the sentence, test it with the first model, and only the positive results are tested on type with the second model \cite[sec. 21.2]{jurafsky2014speech}.

The supervised method has a potential to be very accurate on general inputs, however, to achieve a better results than pattern-based models, a large training dataset is required.
Therefore, the \textit{Distant Supervision for Relation Extraction} was invented to solve the problem \cite{mintz2009distant}.
It is an elegant way, how to obtain a large training dataset.
The DBpedia \footnote{Comprehensive database of structured facts extracted from Wikipedia: \mbox{\url{http://wiki.dbpedia.org/}}} or 
Google Knowledge Graph \footnote{Database of world knowledge structured in a graph: \url{https://www.google.com/intl/bn/insidesearch/features/search/knowledge.html}}
provide enough information about some of the relations between real-world objects.
The algorithm is able to extract full relations with subject, object and relation type.
To gather the training data, the algorithm searches a page about the subject on Wikipedia and finds a sentence with the specified object.
Then, it is ready to adopt the text as an input data for the training phase.
This approach delivers a lot of diverse sentences, in which we are confident about the relation type, and thus the training dataset is rich enough to create a reliable model.

\subsection{Topic Modeling}
\label{topic_modeling}
The \textit{Topic modeling} is a statistical approach for finding general topics that appear in the document.
The keywords are already able to discover some trends of a document, however, unlike the topics, they have to appear in the text.

Prior to the topic recognition, a fixed set of topics have to be defined.
The more advanced algorithms creates a hierarchical topic structure rather than a simple list.
The usual process of topic modeling is to assign probabilities of all topics to every word in the dictionary.
Then, if we consider a document as a bag-of-words
\footnote{Representation of a document that forgets ordering and counts of the words and keeps them in a simple set.}, 
we can compute the overall probability that a document has a specific topic only from its words.

One of the well-known algorithms for the topic modeling is called the \textit{Latent Dirichlet allocation} developed by \citeauthor{blei2003latent} in \citeyear{blei2003latent} \cite{blei2003latent}.
The model assumes that every document has a fixed distribution of topics it belongs to.
Let's further assume that the words of a document were withdrawn from a specific distribution defined in the following way.
\begin{enumerate}
\item Pick a topic distribution of each document according to a \textit{Dirichlet distribution}.
\item Pick a word distribution for each topic.
\item For all words in all documents, generate the word as follows:
\begin{enumerate}
  \item Pick a random topic according to the distribution of topics for the document.
  \item Pick a random word from the distribution of words for the topic chosen in the step a).
\end{enumerate}
\end{enumerate}
The generation scheme tells us the probability theory for documents and topics, however, it does not tell us how to recognize the topics from the fixed documents.
To apply the theory, we need to reverse the process and assume that the documents were created by this generation process, and eventually guess the topic.
One of the iterative methods, which achieves this goal, is called the \textit{Collapsed Gibbs Sampling} \cite{xiao2010efficient}.
Put simply, the initialization phase picks a random topic for each word in all documents and each next phase goes over every word and changes its topic by computing the probabilities, assuming that all the other words are correctly labeled.



\subsection{Document Category}
There are two types of categories we are interested in.
The first is more related to the structure of the text and tells us where the document is coming from. 
Such types can be for instance email, medical record, police record, affidavit, etc.
The Legato system is prepared to work with such categories, and therefore, we rely on the input to contain the information about the first type.

The second type is related to the meaning of the text.
Semantically, it is very similar to the topics introduced in the section \ref{topic_modeling}, and the only difference is that we need the most dominant topic to be chosen as a representative.

\section{Introduction of the tools}

\subsection{NLTK: The Natural Language Toolkit}
As a baseline for the commercial NLP tools, we included the open source python framework \textit{NLTK} \cite{bird2004nltk}.
Although it was originally developed for educational purposes, it is now broadly used by students, researchers and public to process the textual data in python.

\textit{NLTK} consists of multiple modules, which can be used as a pipeline during the text processing. 
The basic modules run the tokenizer, POS tagging, and syntactic analysis.
More advanced modules are dealing with the information extraction and knowledge representation.
The framework comes with large preprocessed corpora that can be used as reference corpora during the keyword or NE extraction \cite{bird2009natural}.

The commercial NLP tools do not need any settings since they are already configured and the configuration is basically the key to their success.
Nevertheless, the \textit{NLTK} is more a framework than a tool and we have to take the time to set the variables right.
It also brings a great advantage of highly configurable settings.
In the next two paragraphs, we explain our approach to extracting the keywords and entities in the \textit{NLTK}.

We applied the \textit{TF-IDF} algorithm to compute the keywords, as explained in \ref{algorithms_keywords}.
Firstly, we removed the stop words and filtered only meaningful words.
The list of stop words was used as defined by the \textit{Scikit-learn library} \footnote{Machine learning library for python \cite{pedregosa2011scikit}}.
To remove the nonsense words, we compared them against a dictionary of the \textit{Brown corpus}, which is the first text corpus of American English of about 1 million words \cite{brownCorpus}.
Next, we filtered only nouns and finally, computed the \textit{TF-IDF} score against the \textit{Reuters} reference corpus \cite[sec. 2.1]{bird2009natural}, which contains more than 10,000 news articles with 1.3 million words.
We have chosen the journalistic context because it was the closest domain to the legal environment.
This procedure outputs a score for each word in a document and the final keywords are those, which \textit{TF-IDF} score was more than a threshold of 0.15.

The entity extraction requires fewer settings since the NE tagger is already trained by the library \cite[sec. 7.5]{bird2009natural}.
We split the text into sentences and consequently sentences into tokens \footnote{token = word as a string without a disambiguated meaning}.
Next step is to extract the part-of-speech tags for each token.
The tokens together with the \textit{POS tags} are the input data into the \textit{NE chunker}, which is able to mark words as named entities and also determine its type.


\subsection{Google Cloud Natural Language}
Google has a long-standing experience with the text processing, especially in the field of information retrieval.
They decided to provide their knowledge of semantic text analysis in a service called Cloud Natural Language \cite{googleNLP}.
Google makes no secret of their approach: the same Deep Learning models that power the Google Search are also employed in the NLP service.

The tool is able to recognize entities, syntactic structure of sentences, category and sentiment of the text.
The sentiment analysis is recently very interesting topic because companies want to know public opinion of themselves.
However, the sentiment in legal documents is usually neutral and does not play a key role in the relevance classification, therefore we decided not to consider sentiment as a feature.

The \textit{Cloud Natural Language} tool also adds a level of confidence to every entity and category so that the user see how strong each feature is. Moreover, the tool replies with a Wikipedia article for every entity, if exists. Another useful feature is the Translation API, which converts text among many different languages and is able to recognize sources of an unknown language. Worth noting that the input does not have to be a pure text but also speech in the audio format or text in a picture.

\subsection{Watson Natural Language Understanding}
In March 2017, the IBM announced retirement of the \textit{AlchemyAPI} service, which was designed to understand the semantics of text and image by advanced techniques \cite{alchemyRetirement}.
One component of the service was also the \textit{AlchemyLanguage}, providing all NLP features, such as entity, sentiment and topic recognition.
As a replacement, a new service emerged: the \textit{Natural Language Understanding} service \cite{watsonNLP}.

Besides typical NLP features, such as keywords, entities, relations, category, sentiment or topics (which are called concepts in the \textit{NLU}), the service provides emotion recognition (joy, anger, fear, etc.) and metadata recognition.
The latter includes basic information about the author, title, prominent page image, and publication date.
Unfortunately, this feature is available only for HTML pages, which is usually not the type of our documents, hence we will not take advantage of the metadata recognition.

One of the most promising features of the \textit{NLU} service is the model customization.
All the NLP features are always recognized with respect to some general language model, which consists of words from all aspects and domains of our language.
This might be a disadvantage, especially when our documents are actually always of one type and contains words from a specific domain.
For instance, the general model would always consider law, justice or  sentence as keywords in our legal documents, however, they might appear in most of the documents and become not relevant anymore in the legal domain.
The \textit{NLU} service enables a user to create its own language model and load it into the cloud.
More than one model can be active and the user can switch between them before each query.

\subsection{Aylien}
\textit{Aylien} is a software package of information retrieval, machine learning, and natural language processing \cite{aylienNLP}, which provides competitive features to the mentioned NLP services from Google and IBM.

In addition to the classical NLP features, \textit{Aylien} provides summarization, which essentially picks the most important sentences from the text. In fact, this feature makes a good sense in our legal domain. Next innovative feature looks for related phrases, which are semantically as close to the original phrase as possible. Another advantage of \textit{Aylien} is that it includes time-based entities in the named entity recognition, specifically, dates. The previously mentioned services surprisingly ignore the time and date expressions most of the time.

Next interesting feature of \textit{Aylien} is the hashtag suggestion, however, it is basically the topic modeling as we described it in \ref{topic_modeling}.
In the same manner, the article extraction is another term for the IBMs metadata recognition.
Truly new feature is the aspect-based sentiment analysis, in which the sentiment is recognized for each aspect, not generally for the whole text.
For example, an analysis of hotel reviews can discover angry opinions on Wi-Fi signal but an excellent rating of the staff.

\section{Comparison}
To compare the tools, we had to restrict the evaluation set only to the common features included in all the tools.
Specifically, the \textit{keywords} and \textit{named entities}: persons, organizations, and locations.
Without any comparison, we also evaluated the relations, which were supported only by \textit{Watson NLU}, and dates supported only by \textit{Aylien}.

\subsection{Testing data: Legal mock case}
Of course, we could not use any real-world case for our testing and analysis due to privacy reasons, therefore, we analyzed a legal mock case called \textit{Davis v. HappyLand Toy Company}\cite{american2015davis} created by the \textit{American Mock Trial Association}\footnote{http://www.collegemocktrial.org/}.

Briefly described, the case puts a father of a child, who was killed by swallowed part of a toy, against the company that produced the dangerous toy. For our use case, we picked five most interesting and distinct documents: 
\begin{enumerate}
\item An email of an inter-company communication about the substances used in the toy.
\item Affidavit (testimony) of the father.
\item Affidavit of the babysitter, who looked after the child at the moment of the fatality.
\item Medical record from the autopsy of the child.
\item Journal paper about the origin, behavior, look, and impacts on a human body of the substance used in the toy.
\end{enumerate}

\subsection{Survey}
The survey was created in a way that no knowledge of the law is necessary, the questions are simple and clearly stated.
Some of the documents are several pages long, hence the survey can be saved and finished later.
The respondents were not provided with any extra information about the legal case so that the results are comparable with the NLP tools, which have no information as well.
The survey was completed by five developers of the Legato system.

We asked the same set of questions for every document.
First two questions asked about the keywords.
We have split the one-word phrases and multi-word phrases and required both with at least 10 one-word keywords.
Next, the respondent has to type people, organizations, and locations that appear in the text.
Moreover, we require relationships between people and dates together with a label specifying which event is related to the date.
Next two questions were related to the semantics and were not used to evaluate the tools.
We asked the respondents to copy a short passage from the text that could be used as an evidence at the court.
The second question asks for a short summary of the whole document.
Both of the questions will be later applied in the training and evaluation of the models
for document relevancy.


\subsection{Statistical comparison}
\label{sec:nlp:statistical_comparison}
\begin{figure}
\centering
\caption{Jaccard index of the NLP tools evaluated against the survey with strict string comparison.}
\label{fig:jaccard_table_strict}
\footnotesize
\begin{tabular}{|l||l||*{5}{c|}}\hline
\rowcolor{orange!50}
\makebox[4em]{\textbf{Feature}}&\backslashbox{\textbf{Document}}{\textbf{Tool}}
&\makebox[3em]{\textbf{NLTK}}&\makebox[3em]{\textbf{Google}}
&\makebox[3em]{\textbf{Watson}}&\makebox[3em]{\textbf{Aylien}}\\\hline\hline
Keywords & Miller affidavit &0.11&&0.11&0.02\\\hline
& Davis affidavit &0.06&&0.03&0.03\\\hline
& Email &0.05&&0.24&0.20\\\hline
& Toxicology paper &0.00&&0.02&0.03\\\hline
& Medical record &0.09&&0.04&0.05\\\hline\hline
& \textbf{Average} &\textbf{0.06}&&\textbf{0.09}&\textbf{0.07} \\\hline\hline
People & Miller affidavit &0.00&0.29&0.54&0.38\\\hline
& Davis affidavit &0.00&0.13&0.25&0.24\\\hline
& Email &0.29&0.44&1.00&0.43\\\hline
& Toxicology paper &0.00&0.00&0.29&0.00\\\hline
& Medical record &0.00&0.27&0.71&0.57\\\hline\hline
& \textbf{Average} &\textbf{0.06}&\textbf{0.23}&\textbf{0.56}&\textbf{0.32}\\\hline\hline
Organizations & Miller affidavit &0.07&0.40&0.50&0.29\\\hline
& Davis affidavit &0.06&0.17&0.33&0.17\\\hline
& Email &0.00&0.25&0.50&0.00\\\hline
& Toxicology paper &0.00&0.33&0.33&0.14\\\hline
& Medical record &0.00&0.00&0.00&0.00\\\hline\hline
& \textbf{Average} &\textbf{0.03}&\textbf{0.23}&\textbf{0.33}&\textbf{0.12}\\\hline\hline
Locations & Miller affidavit &0.00&0.06&0.00&0.00\\\hline
& Davis affidavit &0.00&0.00&0.00&0.00\\\hline
& Email &0.00&0.00&0.00&0.00\\\hline
& Toxicology paper &0.00&0.00&0.00&0.00\\\hline
& Medical record &0.14&0.00&0.00&0.00\\\hline\hline
& \textbf{Average} &\textbf{0.03}&\textbf{0.01}&\textbf{0.00}&\textbf{0.00}\\\hline\hline
\end{tabular}
\normalsize
\end{figure}

\begin{figure}
\centering
\caption{Jaccard index of the NLP tools evaluated against the survey with subset tolerance in string comparison.}
\label{fig:jaccard_table_subset}
\footnotesize
\begin{tabular}{|l||l||*{5}{c|}}\hline
\rowcolor{orange!50}
\makebox[4em]{\textbf{Feature}}&\backslashbox{\textbf{Document}}{\textbf{Tool}}
&\makebox[3em]{\textbf{NLTK}}&\makebox[3em]{\textbf{Google}}
&\makebox[3em]{\textbf{Watson}}&\makebox[3em]{\textbf{Aylien}}\\\hline\hline
Keywords & Miller affidavit &0.30&&0.47&0.57\\\hline
& Davis affidavit &0.35&&0.26&0.33\\\hline
& Email &0.14&&0.57&0.50\\\hline
& Toxicology paper &0.09&&0.37&0.25\\\hline
& Medical record &0.23&&0.24&0.15\\\hline\hline
& \textbf{Average} &\textbf{0.22}&&\textbf{0.38}&\textbf{0.36} \\\hline\hline
People & Miller affidavit &0.79&0.32&0.85&0.94\\\hline
& Davis affidavit &0.64&0.29&0.63&0.47\\\hline
& Email &1.00&0.56&1.00&0.71\\\hline
& Toxicology paper &0.00&0.06&0.29&0.29\\\hline
& Medical record &0.57&0.40&0.71&0.83\\\hline\hline
& \textbf{Average} &\textbf{0.60}&\textbf{0.32}&\textbf{0.69}&\textbf{0.65}\\\hline\hline
Organizations & Miller affidavit &0.36&0.40&0.50&0.71\\\hline
& Davis affidavit &0.06&0.17&0.33&0.17\\\hline
& Email &0.50&0.50&1.00&0.50\\\hline
& Toxicology paper &0.29&0.33&0.33&0.29\\\hline
& Medical record &0.16&0.00&0.00&0.00\\\hline\hline
& \textbf{Average} &\textbf{0.27}&\textbf{0.28}&\textbf{0.43}&\textbf{0.33}\\\hline\hline
Locations & Miller affidavit &0.27&0.61&0.67&0.29\\\hline
& Davis affidavit &0.00&0.10&0.00&0.00\\\hline
& Email &0.00&0.00&0.00&0.00\\\hline
& Toxicology paper &0.00&0.00&0.00&0.00\\\hline
& Medical record &0.14&0.00&0.00&0.00\\\hline\hline
& \textbf{Average} &\textbf{0.08}&\textbf{0.14}&\textbf{0.13}&\textbf{0.06}\\\hline\hline
\end{tabular}
\normalsize
\end{figure}

Since the survey was completed by only five respondents, we decided to unify results for each answer.
The standard approach how to compute agreement between two sources is the \textit{Cohens kappa measure} \cite{smeeton1985kappa}.
The problem with \textit{kappa measure} is that the sources assign a fixed number of items to classes, which is not our situation.
In our case, the number of items is variable and we do not have any classes.
We care about the agreement between the sets.
Therefore, we are using a measure for the distance between sets, the \textit{Jaccard index} \cite{levandowsky1971distance}, defined as follows:
$$
J(A, B) = \frac{|A \cap B|}{|A \cup B|}
$$
Since the union is always larger than or equal to the intersection, we get $0 \leq J(A, B) \leq 1$ for any sets $A$, $B$. 
When $J(A,B)=1$, the sets are identical and when $J(A,B)=0$, the sets do not share any item.

The next step is to define when two items (keywords and entities) are equal.
We have defined two approaches in comparing the equality: \textit{strict approach}, and \textit{subset tolerance approach}.

In the \textit{strict approach}, all items are converted to lower case, so that the names, organizations, locations and other items are compared regardless of the capital letters.
Apart from that, no changes are made to the items and the comparison is based on a strict string equality.
As a consequence, for instance, full name and given name alone are not equal, even if the text represents the same person. 
E.g. for sets $A=\{\text{"Joey Davis", "Brett"}\}$ and $B=\{\text{"Joey", "Brett"}\}$ the Jaccard index is 
$$
J(A, B) = \frac{|\{\text{"Brett"}\}|}{|\{\text{"Joey Davis", "Joey", "Brett"}\}|} = 1/3
$$
Figure \ref{fig:jaccard_table_strict} shows \textit{Jaccard indices} using the strict approach for each combination of tool, feature and document compared with the ground truth, which is the survey output.
The "\textbf{Average}" row displays the mean over all documents for each tool and feature.

The \textit{subset tolerance approach} converts the items to lower case in the same manner as the \textit{strict approach}.
Then, two items are equal if one is a subset of the other or they are strictly equal.
To keep the construction of unions and intersections simple, we include both superset and subset in the union as well as in the intersection.
E.g. for sets $A=\{\text{"Joey Davis", "Brett"}\}$ and $B=\{\text{"Joey", "Davis"}\}$ the Jaccard index is 
$$
J(A, B) = \frac{|\{\text{"Joey Davis", "Joey", "Davis"}\}|}{|\{\text{"Joey Davis", "Joey", "Brett", "Davis"}\}|} = 3/4
$$
Results of the \textit{subset tolerance approach} are displayed in the figure \ref{fig:jaccard_table_subset} in the same form as in \ref{fig:jaccard_table_strict}.
One can notice that the score is always equal or higher compared to the \textit{strict approach}, which is a natural consequence of the approach definitions.

\subsubsection{\textbf{Keywords}}
All tools except for the \textit{Google Cloud Natural Language} service provide the keywords extraction.
The \textit{Jaccard index} is generally very low in the case of the keywords.
There is usually a lot of words to choose from and even the respondents agreed only on few of them.
We can notice the best scores in the case of the email due to the short length of the text.
The \textit{Watson NLU} achieved the best score, however, it did not perform much better than our baseline \textit{NLTK}.
\textit{Aylien} scores high in the \textit{subset tolerance approach} because it extracts very long keywords, which have a high probability of being superset of some of the real keyword.

\subsubsection{\textbf{People}}
The scores for the people extraction were surprisingly high.
In the \textit{strict approach}, the \textit{NLTK} failed because of its limitation of recognizing only one-word items.
People names mostly appeared with both given and family name and the \textit{NLTK} recognized the name separately as two items.
On the other hand, when the subsets are tolerated, the \textit{NLTK} achieved very good result.
The winner of this category is the \textit{Watson NLU}, which is superior for every document and approach.
\textit{Aylien} service achieved a better score than \textit{Google CNL}, especially in the case of the longer documents.

\subsubsection{\textbf{Organizations}}
The \textit{NLTK} achieved the lowest score for the same reason as in the case of the people extraction since the names of organizations also consist of more than one word.
In the strict variant, all other tools performed significantly better than the baseline, yet the best score again achieved the \textit{Watson NLU} service.
In this category, \textit{Google CNL} outperformed the \textit{Aylien} service in the \textit{strict approach}, however, we can see reversed result in the \textit{subset tolerance approach}.
One of the possible explanations is the fact that \textit{Google} is able to precisely determine borders of the entities but they are not so often common with the survey output.
Whereas \textit{Aylien} usually fails at determining the borders since the organizations often consist of many words but the items contain the right information in the subsets.
The Medical record, unfortunately, does not contain many organizations, therefore none services were successful.

\subsubsection{\textbf{Locations}}
The extraction of the locations was quite chaotic and diverse by the respondents as well as by the tools.
The location can be understood as a named entity which is unique in the world, such as Hype park in London, 107 Myers street or Moscow, whereas someone understands the location as any expression specifying the environment, such as living room, school or a car.
The latter type was mostly recognized by the \textit{Google CNL} and the other tools recognized the first type.
Moreover, there were not enough location entities in the documents to draw a meaningful conclusion from the results.
In the strict variant, we ascribe win of the \textit{NLTK} service to a coincidence, as its simple model extracted many words starting with capitals, hence more people than locations appeared in the output set.
In the subset tolerance variant, \textit{Google CNL} and \textit{Watson NLU} achieved very similar score, however, the Watson generated approximately ten times fewer items, which brings us to a conclusion that the discriminative power of the Watsons items might be higher.

\subsubsection{\textbf{Dates}}
As the \textit{Aylien} service is the only one that is able to extract the time-based entities, we have not performed any comparison.
Still, we wanted to see how precise is the extraction, and so we present the \textit{Jaccard index} computed from the \textit{Aylien} output and the survey data.

\begin{figure}[h]
\centering
\caption{Jaccard index of the time-based entities extracted by Aylien.}
\label{fig:jaccard_dates}
\footnotesize
\begin{tabular}{|l||*{5}{c|}}\hline
\rowcolor{orange!50}
\makebox[4em]{\textbf{Document}}&{\textbf{Jaccard index}}\\\hline\hline
Millers affidavit & 0.07 \\\hline
Davis affidavit &0.13\\\hline
Email &0.00\\\hline
Toxicology paper &0.00\\\hline
Medical record &0.00\\\hline\hline
\textbf{Average} &\textbf{0.04}\\\hline
\end{tabular}
\normalsize
\end{figure}
The \textit{Aylien} achieved non-zero results only in the longer documents and even then, the scores are under our estimation.
As an example, a simple date in form "24. 12. 2017" is recognized as a phone rather than a date.
Here, it strongly depends on the spaces: after we remove a space before the month -- "24.12. 2017" -- the day and month are finally recognized as a date, however, the whole string is still reported as a phone.
On the other hand, we took into consideration only the dates, while \textit{Aylien} recognizes times and other expressions, such as "last week", as well.

\subsection{Analytical comparison}
Aside from the statistical evaluation of the tools accuracy, it is also important to manually analyze the output and check whether it is semantically correct.
In this section, we discuss characteristics of the tools and their benefits and drawbacks.

\subsubsection{\textbf{NLTK}}
We already outlined two drawbacks of the \textit{NLTK} in the previous section: one-word limitation and simple NE model.
The first problem is visible in both affidavits, where the toy name appears under the name "Princess Beads", which is an obvious keyword in this context.
Interestingly, the \textit{NLTK} extracted both parts: "princess" and "beads" as keywords since the words are quite unique by itself.
However, the key phrase was missed.
As the \textit{NLTK} is highly customizable, we believe that the multi-word phrases extraction is indeed possible but it would require an extra amount of work.

On the other hand, the keyword extraction performance was very well comparable to the commercial tools.
\textit{NLTK} performed considerably worse in the entity extraction.
Besides the separation of the given and family name, it also falsely marked "Liquid", "Myth" or "Identification" as people, probably because of the capital letters.
One can notice a lot of false positives in the Medical record, were a lot of words were written in the capitals and \textit{NLTK} recognized all of them as organizations, e.g. "BODY", "AND" or "OF".

Although the \textit{NLTK} provides a very good baseline for the NLP feature extraction, we can see a noticeable line between its hand-written patterns and the supervised machine learning models, which are used by the other tools.

\subsubsection{\textbf{Google Cloud Natural Language}}
The approach of the Google is a little different in the following way.
Together with the entity, it outputs also a position in the text and it does not discard duplicates.
\textit{Google} does not understand the extraction as picking of the most identifying words, but more as a highlighting of the words in the text.
Therefore, the \textit{Google CNL} usually produces a larger output than others.
The true positive rate is almost perfect, for example in the Millers affidavit, all of "Brett Miller", "Lee Davis", "Andy Davis", "Hillary Davis" and "Joey Davis" were recognized.
Even more surprising is that "Hillary Davis" appears only once in the text and \textit{Google} was able to link the name to many occurrences of her given name.
The problem is that words, such as "kids", "one", or "friend" are recognized as well, and truth is that they really represent some people in the meaning, however, it is not necessary and rather disturbing in our use case.

\subsubsection{\textbf{Watson Natural Language Understanding}}
The \textit{Watson} approach is slightly more moderate.
Very little false positives are introduced as \textit{Watson} outputs a word only in case of high confidence.
Similarly as \textit{Google}, \textit{Watson} recognized all people in the Millers affidavit including full name of "Hillary Davis".
Due to the conservativeness, only the family name of "Chase Tuchmont" was recognized since "chase" is an ordinary word in English as well.
However, the overwhelming majority of the recognized entities makes perfect sense and are truly people, organizations or locations.
Therefore, the \textit{Watson NLU} service is not only superior in the statistical evaluation, but also in the analytical overview of the semantics.

\subsubsection{\textbf{Aylien}}
We can see yet another approach by the \textit{Aylien} service.
Keywords are understood as complex pieces of information of arbitrary length, sometimes even more than 10 words long.
As a result, service correctly extracted "Joye" as a keyword in the Davis affidavit, though incorrectly extracted a key phrase "Davis and that I wanted to speak with someone about Princess", which seems rather awkward.
This was a source of a low score of \textit{Aylien} in the keywords category.

On the other hand, \textit{Aylien} excelled in the extraction of people.
Again, all people in the Millers affidavit were successfully recognized, even though the model was not sophisticated enough to merge "Hillary" and "Hillary Davis".
Despite a few mistakes, mostly recognizing numbers as people, the service was able to keep high true positives and low false positives, including organizations and locations.


\chapter{Classification}
Data exploration in the chapter \ref{chp:data_characteristics} helped us clarify the language and style of the legal documents enough to design models for the relevance classification.
% ... write about the outcomes of data characteristics chapter when they're done ...

First section (\ref{sec:attributes}) presents how the NLP features are encoded into attributes applicable to the models.
Next sections describe the system of models that predicts the document relevance.
Due to the large number of features, we decided to create a layered system of models, where the top-level models make predictions based on the outputs of the bottom-level models.
The section \ref{sec:bottom-level-models} introduces the bottom-level models specialized on each feature separately, that are able to understand the word embeddings.
Next, the top-level layer is formed by a pipeline of three machine learning models that are consecutively making decisions about the documents to output the final relevance.
First is the \textit{Law-Case classifier} \ref{sec:law-case-classifier}, second is the \textit{Sentence classifier} \ref{sec:sentence-classifier} and the last is the \textit{Document classifier} \ref{sec:document-classifier}.
Each of them needs a different set of inputs and applies different machine learning model.
The following figure visually explains the system of models.
For further details about the individual classifiers, see the related sections.

% TODO write that there'll be a lot of claims proven in evaluation section

\begin{figure}[H]
\caption{Diagram of models}
\label{fig:models-diagram}
TBA
\end{figure}

\section{Attributes}
\label{sec:attributes}
To achieve a successful classification, one of the most crucial steps is to convert the data into suitable attributes, which will be most helpful in determining the classes.
Technically, all machine learning models are built to process numbers as input variables.
Since we work here with text and only text, we have to find a way how to convert all keywords, entities, relations, syntactic structure, and other features into numerical representation.
During this process, we have to keep in mind the main purpose: the numbers needs to internally hide information about the relevance.

Based on the arguments made in the chapter \ref{chp:data_characteristics}, the units of the machine learning are sentences rather than documents.
This fact implies that the attributes are related to a sentence.
As a result, there are less data from which to compute the attributes and the representation of the attributes as well as the classification itself become simpler.

\subsection{Sentence meaning embedding}
\label{sec:sentence-meaning}
When extracting a meaning of a sentence, one will encounter two major problems to be solved: 1) how to represent meaning of a single word 2) how to combine word meanings.

The first problem is also known as the \textit{Word Embedding} problem \cite{wordEmbeddings}.
The simplest solution is to assign unique numbers to all words and train classifiers on the numbers instead of words.
$$
\{\text{"knife"} \rightarrow 1, \text{"gun"} \rightarrow 2, \text{"wire"} \rightarrow 3, \dots\}
$$
This approach does not work since the classifiers interpret a number as a quantitative, not nominative,  attribute.
As a result, the classifier understands two words with close numbers as close by the meaning as well.
Since the numbers were assigned without any order, there is undoubtedly a lot of misinterpretation.

The next solution is often applied in the information retrieval systems.
Instead of one number, we can represent the word embedding by a vector, with all elements zeroed but one.
Each word is assigned an unique index, on which the vector keeps a value of one.
$$
\{\text{"knife"} \rightarrow (1,0,0), \text{"gun"} \rightarrow (0,1,0), \text{"wire"} \rightarrow (0,0,1), \dots\}
$$
Now, a vector uniquely identifies a word and the vectors are not comparable for the classifier, which eliminates the problem with the previous approach.
This solution can be further improved by counting the words in the training documents, TF-IDF weighting or co-occurrence matrices \cite{wordEmbeddings}.

The third and final solution goes even further in the embedding of the semantics.
Recent research by Mikolov et al. \cite{mikolov2013efficient} introduced a revolutionary algorithm \textit{Word2vec}, which uses a neural network to create a vectors of fixed length with its semantics compressed.
Each dimension of a vector represents some concept, which can be somehow quantified, such as masculinity, age or liveliness \cite{mikolov2013linguistic}.
The vectors are capturing semantic regularities, and therefore, the classifiers have a potential to perform better even for unknown words.
This embedding ensures our ultimate assumption, that close vectors are also close by the meaning of their underlying words.

Both the algorithm and the trained vectors are publicly available.
Though we could use the algorithm to train our own vectors, it is much easier to adopt the already trained dataset.
Since the meaning of the words is the same regardless of the area, there would be a very little gain in the training a custom dataset specialized on legal documents.
The dictionary we will use is trained by the \textit{Google} company and is available on their official code archive \cite{word2vecGoogle}.
The pre-trained model contains 3 million unique words of 300-dimensional vectors, which was trained on a 300 billion word news dataset.
For our purpose, a trimmed dataset of 300 thousand unique words without the phrases is sufficient \footnote{Downloaded from \url{https://github.com/eyaler/word2vec-slim}}.

The second problem relates to the fact, that the classifiers usually need fixed input, but we currently computed vectors for each words of a sentence.
Researches Vedantam et al. \cite{vedantam2015learning} applied a simple average of \textit{word2vec} vectors to the words in a phrase to get the final embedding.
We can extend the idea to all sentences and achieve one vector per sentence by computing an average of all words in the sentence.
To achieve more interesting results, it is possible to weight the words by their TF-IDF score in the overall average, however, the contribution is not necessary positive since the ordinary words can distinguish relevant sentence as well as the keyword.
Besides, we embedded the keywords (words with high TF-IDF) in another representation, hence the TF-IDF weighting does not bring enough contribution to be worth the processing time.


\subsection{Keyword embedding}
Next attribute is a vector computed from the keywords of a sentence.
The NLP feature extraction determined keywords in the context of the whole documents, which means the first task is to find the working set of words by intersecting the sentence with the keywords.
In the process of intersecting, the \textit{subset tolerance approach} is applied, as defined in the section \ref{sec:nlp:statistical_comparison}.

The next step is very similar to the approach mentioned in the previous section -- the words are converted into embedding by \textit{word2vec} dictionary and then averaged into one 300 dimensional vector.
Indeed, the sentence meaning vectors contain the keywords as well, thus some information is redundant and one can argue that these two attributes appears too similar.
However, the choice of right words plays a crucial role in the influence on classification, and the importance of both the attributes will be obvious in the evaluation chapter.


\subsection{Entity embedding}
\textit{Watson NLU} service is able to recognize extensive number of entity types (462) \footnote{\url{https://console.bluemix.net/docs/services/natural-language-understanding/entity-types-v1.html\#entity-types-and-subtypes-version-1-}}, 
which include as obscure categories as "WebBrowserExtension" or "HockeyConference".
For the purpose of this research, only 6 of them the following types are used:
\begin{itemize}
\item Person
\item Organization
\item Location
\item Crime
\item HealthCondition
\end{itemize}
Note that also "Company" type is recognized, subsequently classified as "Organization".
In addition, the concepts and dates are included as entities, since there is no difference in the way they might influence the relevance.

The first draft of the entities embedding was based only on the entities appearing in the case.
During the case preprocessing, the mentioned entities were extracted from the case synopsis and stored in a vector of fixed length for each entity type. For instance, a vector for people entities might look as follows:
$$
(\text{"joey david", "thomas", "jesse hester", "obama", "", "", "", "", "", ""})
$$
The rest of the vector is left for entities from documents, which would be included based on their importance score (\textit{Watson NLU} attaches the score to each entity).
Finally, the representation is a mask array of ones and zeros that indicates which entities are present in the sentence.

One problem with this embedding is that the vector acquires different meaning for every case.
In a situation when a model learns that a name on the index $i$ indicates a relevant document, the model becomes not transferable to another case, where the vector contains different names.
General model for any case would become impossible to train and even the case-specific models would face a lack of data to achieve a satisfactory performance.
Fortunately, the \textit{word2vec} dictionary was trained on general texts including the entities as well.
Most of the well-known names, such as "David", "George", or "Adele", organizations, such as "Mc Donald", "IBM", or "Facebook" and locations, such as "Prague", "Nigeria", or "Prater" are present in the dictionary.
Not only that we can now train a general model, in which each word has its own fixed representation, but also the embeddings satisfy the expected semantic rules.
For example, vectors representing names will be close together and the model has a chance to learn that names are in fact important regardless of the specific word.


\begin{figure}[H]
\centering
\caption{Distances among entity vectors in word2vec dictionary.}
\label{fig:entity_distances}
\begin{tabular}{|*{5}{c|}}
  \hline
           & David   & George  & IBM      & Facebook \\ \hline
  David    & 0.0     &         &          &     \\ \hline
  George   & 0.93944 & 0.0     &          &     \\ \hline
  IBM      & 1.37978 & 1.41457 & 0.0      &     \\ \hline
  Facebook & 1.37652 & 1.40856 & 1.319643 & 0.0 \\ \hline
\end{tabular}
\end{figure}

TBA - explain the table and make conclusion

\subsection{Relation embedding}
\label{sec:relation_embedding}
The relations computed by Watson NLU keep the following structure:
\begin{listing}[H]
\begin{minted}[frame=single,
               framesep=3mm,
               linenos=true,
               xleftmargin=21pt,
               tabsize=4]{js}
{    
 "type" : "locatedAt",
 "sentence" : "her coming into the courtroom late"
 "arguments" : [
    { "text" : "her", 
      "entities" : [{ 
      	"text" : "her", 
      	"type" : "Person" }] 
    }, 
    { "text" : "courtroom",  
      "entities" : [{ 
      	"text" : "courtroom", 
      	"type" : "Facility" }] 
    }], 
}
\end{minted}
\caption{Watson NLU relation example in JSON} 
\label{json:relation}
\end{listing}
Note that the fields "location" and "score" were excluded to make the example compact.
As the JSON example suggests, the important parts of the relation are its type ("locatedAt"), types of the entities ("Person", "Facility") and the text of the entities ("her", "courtroom").
The relation embedding, therefore, needs to accommodate relation type, 2x entity type and 2x entities itself.
The previous sections presented the \textit{word2vec} dictionary, which is able to convert common words into an vector.
The same approach is used here for embedding of the entity words, since there is a high probability of them being in the dictionary.

On the other hand, types of relations and entities are rather identifiers than meaningful words, and thus, they will hardly occur in the \textit{word2vec} dictionary.
In this case, we can take advantage of the lists of all possible types, which is offered by Watson.
There is a limited number of relation types (54) and entity types (49), which provides a transparent way of converting the type into a vector.
The type is encoded by a vector of length equal to the number of types, in which each index represents one specific type.
Then, the embedding contains zeros in all elements except for the index of the type it represents.
This approach is called \textit{One-hot encoding} as defined by the Scikit-learn machine learning library \footnote{\url{http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html}}.

To summarize, the final embedding of the relation consists of:
\begin{itemize}
\item relation type in one-hot vector (length 54)
\item entity type of the subject in one-hot vector (length 49)
\item entity type of the object in one-hot vector (length 49)
\item the subject in word2vec vector (length 300)
\item the object in word2vec vector (length 300)
\end{itemize}


\subsection{Category embedding}
The category is an example of the attribute related to the document instead of the sentence.
Similarly as the relation and entity types, the Watson NLU defined a fixed set of categories.
There are five levels of categories arranged in a hierarchic tree structure, but not all categories contain a full range of levels.
An example of a category with four levels might be
\begin{minted}[frame=single,
               framesep=3mm,
               linenos=false,
               xleftmargin=-5pt,
               xrightmargin=-5pt,
               tabsize=4,
               fontsize=\small]{js}
law, govt and politics / legal issues / civil rights / privacy
\end{minted}

As the principle of one-hot vector was explained in the previous section (\ref{sec:relation_embedding}), the category embedding should be straight-forward.
Each category level has a fixed number of types, therefore, one-hot vectors are created for all levels and are concatenated in one vector of representing a full category.
It is worth noting that the last (fifth) level is excluded in the embedding, since the absolute majority of categories get by with the first four levels, which left the last level largely unused.

%\subsection{Syntax structure}
\section{Bottom-level model}
\label{sec:bottom-level-models}
This section introduces the model that directly processes the extracted embeddings explained in the previous section and extracts information provided to the top-level models.
To achieve the best performance, it is crucial to split the predictions to elementary units and build on top of them.
% TODO find a source to prove it
The models are able to learn limited amount of information, especially in the case of insufficient number of training instances, and therefore, one should keep the models as simple as possible.
In contrast, one could concatenate all inputs, apply them to a single large model and expect that it will be able to extract all important information.
This approach is not very likely to work in the environment with thousands of features, as in our case.

Following this idea, we preset a series of models specialized on each of the attribute separately.
Besides better final performance in predicting the relevance (\ref{eval:allinone-vs-features}), another advantage is the transparent flow of information and better understanding of the models.
Examining the intermediate results from the models, we are able to deduce which attributes (keywords, entities and relations) are the most discriminative in predicting of the relevance and which are not.

\subsection{Machine learning model selection}
The evaluation chapter contains a test on the most suitable machine learning model for the bottom-level predictions (\ref{eval:bottom-model-selection}).
Besides the fact, that the neural networks achieved the best results, it is worth to explain, why they are naturally the most suitable model.

Regarding attributes describing the sentences, the final solution was to apply the \textit{word2vec} dictionary to create an embedding.
As mentioned in the section \ref{sec:sentence-meaning}, the dictionary was computed from the texts of news reports that are processed by a neural network.
Specifically, in the proposed word2vec algorithm, the vectors are derived from the weights between the hidden and output layer \cite[sec. 2.2.1]{wordEmbeddings}.
Since the embeddings represent an internal state of a neural network, the natural step is to extend the network by virtually connecting additional layers.
Even though the idea of extending network is not precise, as the embeddings are understood as inputs instead of weights, the aspect of the follow-up model structure showed to be important (\ref{eval:bottom-model-selection}).

Another reason, why neural network best suites our needs, is that it is an incremental estimator \cite{incrementalScikit}.
Incremental estimators enables to iteratively learn and improve on a new batch of data without forgetting the old state.
Other incremental estimators solving classification problems include Naive Bayes (NB) or Stochastic Gradient Descent (SGD) learning.

The final structure of the neural network has an input layer of dimension 300, two hidden layers of 6 and 3 neurons and 2 output neurons representing binary classes.
Such a model is prepared to predict law-related vs case-related sentences as well as relevant vs irrelevant sentences. Note that even though there are three final classes of relevance (HOT, WARM, COLD), the WARM class is neglected to be decided by the top-level models.

\begin{figure}[H]
\caption{Bottom-level model structure}
\label{fig:neural-network}
TBA
\end{figure}

The \textit{sigmoid} activation function proved to be the most successful in all layers (\ref{eval:bottom-model-tuning}).
During the model training, an Adamax optimizer with learning rate of 0.02 was applied in 100 epochs and batch size of 10.
To prevent the overfitting, the model stopped learning in case of increasing loss on a validation set (10\% of training data).
The applied loss function is called the Cross Entropy (CE), which is defined as follows:
$$
H(P, Q) = -\EX_{x \sim P} \log{Q(x)} = -\sum_x P(x) \log{Q(x)}
$$
where $P$ is the true probability distribution and $Q$ is the predicted probability distribution over samples $x$ \cite{Goodfellow-et-al-2016}.
Note that the roles of true and predicted distributions are given, in contrast to the accuracy measure.

\section{Law-Case classifier}
\label{sec:law-case-classifier}
The goal of the Law-Case classifier is to assign a document to one of the classes: law-related or case-related.
As the Data Characteristics chapter (\ref{chp:data_characteristics}) defines the classes only loosely, the following paragraphs provide more specific and statistical definition.

Case-related documents contain at least one specific information about the subjects or acts related to the case. 
The authors of these documents are usually directly involved in the case or came into contact with the subjects.
They are mostly confidential, such as emails, medical records, emergency call records, police records or testimonies.
Their main characteristic is an occurrence of a known entity, such as victim, defendant or place of the act.

Law-related documents do not contain any information specific to the case.
They usually state facts that help lawyers argue about the legal aspects of the case and find disagreement and evidence.
Examples of such documents are fragments of law code, description of similar cases from the past, policies of a company/school/office, news, books or articles.
Their main characteristic is a specific vocabulary, usually including the legal terminology.

All 263 documents were labeled by one of the classes as part of the annotation process.
To train the models, we had to establish a simplified assumption and mark all sentences with the same class as the document.
As a result, the models were trained on all 14,368 sentences.

\textit{TBA - some results of the bottom-level model}

The top-level model is based on the two outputs (prob. of case-related and prob. of law-related) from each bottom-level model and encoded document category.
To eliminate the influence of a document length, outputs are averaged for all sentences.
In this case, the model is trained on the set of documents, which has only 263 instances, hence the neural networks are not applicable.
As the best classifier showed to be the \textit{Random Forest} with 300 base estimators and maximum tree depth of 2 (\ref{eval:law-case-model-selection}).

\textit{TBA - some results of the top-level model}

Now, when the division to the two diverse types of documents is in place, a labeled document can proceed to the specialized relevance estimator.

\section{Sentence relevance classifier}
\label{sec:sentence-classifier}


\section{Document relevance classifier}
\label{sec:document-classifier}
[Law-Document and Case-Document classifier]


\chapter{Implementation}
\section{Frameworks and libraries}
\section{Design and implementation}

\subsection{Text preprocessing}
The documents come into the Legato system in one of the two formats: a text in a digital form or an image.
The latter needs to be converted into the former, which is achieved by the Optical Character Recognition\footnote{recognition of text characters from a picture and conversion into a text in a digital form.} (OCR).
Unfortunately, the OCR technique is not perfect and leaves specific artifacts in the text.
Therefore, we first apply the text processing, which filters, edits and removes these artifacts and prepares the text to be segmented into sentences.

Firstly, all empty lines are removed and consecutive white spaces are converted into a single space character.
The structure of a document is lost with this edit, which might mean a loss of some information, however, we decided to focus on the semantics more than the structure.
Secondly, the OCR artifacts has to be removed.
For example, the recognition puts '\&\#39;' instead of the apostrophe and '\&quot;' instead of the quotation.
Another artifact of the OCR are coupled words with missing spaces -- a dictionary of English language is applied to break the words apart.
The final step is a removal of a hyphenation that was introduced to break new lines in the original document.

\section{Integration into Legato}

\chapter{Evaluation}
\label{chp:evaluation}

\section{Decision point evaluation}

\subsection{Topology system of models}
% popsat ze jsem zkusil pouzit features primo z celeho dokumentu, ale uspesnost byla nizsi nez pri pouziti vet a pak dokument

\subsection{Bottom-level model selection}
\label{eval:bottom-model-selection}
% Law-Case classification based on Sentence Meaning
% ...Neural Networks        0.860
% ...SVM C=1.0              0.754
% ...RF n_est=500 depth=20  0.845

\subsection{Law-Case model selection}
\label{eval:law-case-model-selection}
% vyber law-case top-level classifier, vyhral to RF s 300 a 0.86 zatim
% zkusil jsem na zaklade Conceptů - vybrat concepty z case_related a law_related, odebrat ty, co jsou v obou a potom pocitat, kolik konceptu ma dokument spolecnyho s temito mnozinami - jen 0.756, kazdopadne pro law_related ma 98% uspech, protoze 63 z 64 spravne uhadlo, z celkovyho 263 oznacilo 126 jako law_related, takze pridana hodnota je

\subsection{Keyword type selection}
% comparing keywords from case and doc to predict law-case
% Case....0.839 
% Doc ....0.863

\subsection{Number of models}
\label{eval:allinone-vs-features}
% conclude research on whether put all features in one long vector or separate models
% 1. one large model, concatenate all attribtues for sentence (keyword, entities, relations..)
% 2. separate models for attribubtes, plus top-level model, examine relevance or law-case

\subsection{Relations with types}
% relations: acc with full vector with types: 82 \%, acc with only words (300+300) 79\% => types helps

\subsection{LSTM RNN networks}
% tried LSTM RNN networks, which failed to recognize anything, - describe how I fed it, all text of each document

\subsection{All words}
% predict law-case based on training on all 190k words, predict class of word - 0.83

\subsection{Word2vec contribution}
% predict with embedded numbers - use embedding layer of keras with 300 000 as len(dictionary)


\section{Parameter tuning}

\subsection{Bottom-level model}
\label{eval:bottom-model-tuning}
% tuning of the bottom-level NN, i.e. activation functions, optimizers and layers

\subsection{Top-level models}

\section{Results}

\chapter{Conclusion}

% future work
% - Watson NLU enables to create custom language model, which could recognize entities, keywords and relations even more precise if we feed it with legal data. We could not, since we have not enough text, but a future research could work with this.

% - Watson NLU enables 462 entities - possible to include all of them

% Matter of privacy - how to ensure the models do not enable to retrieve information about other case

% case lookup - find similar cases

  \printbibliography[heading=bibintoc] %% Print the bibliography.

  \makeatletter\thesis@blocks@clear\makeatother
  \phantomsection %% Print the index and insert it into the
  \addcontentsline{toc}{chapter}{\indexname} %% table of contents.
  \printindex

\appendix %% Start the appendices.
\chapter{An appendix}
Here you can insert the appendices of your thesis.

\end{document}
