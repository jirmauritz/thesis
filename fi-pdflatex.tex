%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this work into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  digital, %% This option enables the default options for the
           %% digital version of a document. Replace with `printed`
           %% to enable the default options for the printed version
           %% of a document.
  table,   %% Causes the coloring of tables. Replace with `notable`
           %% to restore plain tables.
  lof,     %% Prints the List of Figures. Replace with `nolof` to
           %% hide the List of Figures.
  lot,     %% Prints the List of Tables. Replace with `nolot` to
           %% hide the List of Tables.
  %% More options are listed in the user guide at
  %% <http://mirrors.ctan.org/macros/latex/contrib/fithesis/guide/mu/fi.pdf>.
]{fithesis3}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the T2A font encoding
\usepackage[T1,T2A]{fontenc}  %% to use the Cyrillic fonts with Russian texts.
\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
  english, german, russian, czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%
%%   \begin{otherlanguage}{german}  ... \end{otherlanguage}
%%   \begin{otherlanguage}{russian} ... \end{otherlanguage}
%%   \begin{otherlanguage}{czech}   ... \end{otherlanguage}
%%   \begin{otherlanguage}{slovak}  ... \end{otherlanguage}
%%
%% For non-Latin scripts, it may be necessary to load additional
%% fonts:
\usepackage{paratype}
\usepackage{textcomp} % to be able to use normal apostrophe with \textquotesingle
\def\textrussian#1{{\usefont{T2A}{PTSerif-TLF}{m}{rm}#1}}
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{
    date          = 2018/05/20,
    university    = mu,
    faculty       = fi,
    type          = mgr,
    author        = Jiří Mauritz,
    gender        = m,
    advisor       = Ing. Libor Kubečka, 
    title         = {Automatic Classification of Legal Documents},
    TeXtitle      = {Automatic Classification of Legal Documents},
    keywords      = {machine learning, legal documents, natural language processing},
    TeXkeywords   = {machine learning, legal documents, natural language processing},
    abstract      = {This is the abstract of my thesis, which can

                     span multiple paragraphs.},
    thanks        = {These are the acknowledgements for my thesis, which can

                     span multiple paragraphs.},
    bib           = example.bib,
}
\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.
%% These additional packages are used within the document:
\usepackage{paralist} %% Compact list environments
\usepackage{amsmath}  %% Mathematics
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}      %% Hyperlinks
\usepackage{markdown} %% Lightweight markup
\usepackage{listings} %% Source code highlighting
\lstset{
  basicstyle      = \ttfamily,%
  identifierstyle = \color{black},%
  keywordstyle    = \color{blue},%
  keywordstyle    = {[2]\color{cyan}},%
  keywordstyle    = {[3]\color{olive}},%
  stringstyle     = \color{teal},%
  commentstyle    = \itshape\color{magenta}}
\usepackage{floatrow} %% Putting captions above tables
\usepackage{slashbox} %% Big slash in tables
\usepackage{cleveref}[2012/02/15] %% Enables cross reference footnotes
\floatsetup[table]{capposition=top}

\begin{document}
\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

Theses are rumoured to be the capstones of education, so I decided
to write one of my own. If all goes well, I will soon have a
diploma under my belt. Wish me luck!

\chapter{Background: Legato System}

\chapter{Related work}

\chapter{Data Characteristics}
\label{chp:data_characteristics}
One of the most important preconditions for building an efficient model is to understand the input data characteristics.
People developed numerous models based on text classification that are suppose to classify text of any domain, however, at the expense of the efficiency.
The specialized models always have the potential to outperform the general one.
The real contribution of this work is the understanding of characteristics of the legal domain data and its application on the process of building a text classification model.
This chapter will introduce our working dataset, its statistics and characteristics.

\section{Dataset introduction}
The end users of the Legato system are mostly the U.S. lawyers, which influences our choice of the dataset.
The U.S. legal system is very specific with its own laws, constitution, trial process and most importantly for us -- terminology.
Also, we have to take into consideration specifics in the form of the documents: medical records, police records, affidavits, emergency call transcripts etc.
We do not claim that our application will not work for other than U.S. legal documents, however, the model might manage to recognize the structure and terminology of the U.S. documents more accurately.

Unfortunately, we have not acquired enough training data from the users of Legato, mainly because the legal cases are ongoing and the documents are too confidential to be included in a research.
For the same reason, it is hard to find an authentic public trial documents on the Internet.
We have found a solution thanks to an \textit{American Mock Trial Association} \footnote{http://www.collegemocktrial.org/} and \textit{Classroom Law Project} \footnote{\url{http://www.classroomlaw.org}}.
Both associations create legal cases for educational purposes, which then serve as a material for the mock trial competitions.
All their legal cases are fictional, though some of them were inspired by real-world situations and issues.
Still, any resemblance to actual persons and events is purely coincidental.

All 20 cases used as a training set are downloaded from two sites: Street Law, inc. \footnote{\url{http://streetlaw.org}} and Classroom Law Project \footnote{\url{http://www.classroomlaw.org}}.
Not to be biased with one legal area, we included cases of diverse legal fields:
\begin{itemize}
\itemsep0em
\item[2x] \textbf{School environment} \\ educational malpractice or indifference
\item[1x] \textbf{Bullying} \\ intentional infliction of emotional distress
\item[2x] \textbf{Criminal law} \\ assault, battery or murder
\item[3x] \textbf{Inmate partner violence} \\ domestic violence by a current or former partner
\item[3x] \textbf{First Amendment} \\ breach of freedom of speech, religion, press etc.
\item[4x] \textbf{Child neglect} \\ child injury or death caused by parents, baby-sitter or teacher
\item[2x] \textbf{Sex crime} \\ sexual harassment and disease transmissions
\item[3x] \textbf{Tort law } \\ fault leading to suffer or harm, without breaching any contract
\end{itemize}
All mock cases have a similar structure: there are approx. 5-8 testimonies of the involved people, called affidavits, and the rest of the documents are emails, medical records, police reports, emergency call transcripts, description of the past similar cases etc.
The affidavits are up to 10 pages long, usually describing the issue in depth, whereas the other documents are usually one or two pages long.
We excluded all extra information specific to the mock trials, explanations of the competitions and summaries: we do not expect these to be typical part of a case.
Some extra headers and footers that contain metadata were also removed not to confuse the model training.

During the model building, we need to take into account that the real cases might be more or less different from the mock cases.
For instance, we do not expect the affidavits to appear in the real cases as much as in the mock cases.
Also, we expect more emails and papers of unknown structure in real cases.
Generally, we found two major shifts: 
\begin{enumerate}
\item real cases will most probably contain shorter documents
\item real cases will contain many more unrelated documents
\end{enumerate}
We present solutions for both issues:
\begin{enumerate}
\item 
The length of the document should be an insignificant parameter when determining the documents relevancy.
Therefore, we decided to focus on sentences rather than on a document as a whole.
In other words, we determine relevance of the document by exploring the relevances of the individual sentences.
As a result, the model is slightly simplified, however, we do not completely ignore the cross-sentence information, as we will see in the further chapters.
Thanks to the sentence-based model, we should be able to eliminate an influence of the document length and judge the relevance without bias.

\item
The second issue is caused by the lack of motivation to add unrelated documents during the creation of the mock cases.
Still, we can find some documents that are related to the case but are not relevant, e.g. resume of a defendant.
A straight forward solution is to add some irrelevant documents from other cases to simulate the balance in a real situation.

\end{enumerate}

\section{Annotation}
- num of annotators
- annotation of docs
- annotation of sentences

\section{Data statistics}
As mentioned earlier, we are working with 20 cases with total of 261 documents, which means 13.15 documents per case in average.
The documents together contain 234,798 words, thus average number of words per document is 900.
We counted 14,518 unique words which is approx. 1.18 \% of English vocabulary according to the \textit{Oxford English Dictionary}  \cite{oxfordDictionaries}.

\begin{figure}[h]
\centering
\caption{Most frequent words in the dataset with and without stop words.}
\label{fig:most_frequent}
\footnotesize
\begin{tabular}{|l|r||l|r|}\hline
\rowcolor{orange!50}
\multicolumn{2}{|c||}{\makebox[9em]{\textbf{All words}}} & \multicolumn{2}{|c|}{\textbf{Without stop words}} \\\hline\hline
\textbf{word} & \textbf{count} & \textbf{word} & \textbf{count} \\\hline\hline
the & 10825 & school & 855 \\\hline
to & 6632 & would & 683 \\\hline
I & 6274 & one & 566 \\\hline
and & 5912 & time & 497 \\\hline
of & 5479 & 1 & 476 \\\hline
a & 4896 & said & 451 \\\hline
in & 3603 & told & 420 \\\hline
that & 3499 & could & 409 \\\hline
was & 2846 & get & 391 \\\hline
for & 1942 & like & 386 \\\hline
\end{tabular}
\normalsize
\end{figure}

In the left part of the figure \ref{fig:most_frequent}, we present counts of the most frequent words in the documents.
It is not a surprise that all top ten words are common stop words of English.
The stop words are usually removed before any further processing of the text, however, they might carry valuable information too.
For instance, recognition of the author or formality level can be determined based on the stop words usage.
We can see that the usage of stop words is not different from the general English.

The right side of the table shows most frequent words after we removed the stop words.
Here, we can see clear influence of the cases from the school environment.
Next, the words "said" and "told" indicate that the documents often contain reporting of events and conversations.
The other words again fit more or less in the general English.

The word frequencies tell us that the vocabulary is not biased by any major trend.
Lets now look at the keywords of the whole collection and discover words that occur more frequently than usual.
The method used is explained in section \ref{algorithms_keywords} and we applied the same reference corpus (Reuters articles).
The following are the keywords: city, school, know, got, joey, just, police, princess, time, person, really, like, child.

- most frequent words for crime vs. first amendment
- most frequent words for relevant vs. unrelevant cases

\section{Types of documents}
- non-sense (pics)
- law related vs. case specific
\section{Linguistics}


\chapter{NLP tool selection}
The amount of text in the legal documents can vary largely.
Most of the documents we are working with stretch across one page, however, the average number of pages is 6 and we can find documents up to hundred pages long.
The typical approach of the text categorization techniques is to create a document-term matrix containing counts of terms in each document.
The problem with this approach is that the long documents are represented by a vector, which \emph{information gain} is rather low.
For us, a high \emph{information gain} means that the representation captures the general idea, the purpose of the document, and similarity/dissimilarity with other documents.

To address the issue, we decided to represent a document with specific features extracted by one or more NLP tools.
We are looking for potentially the most interesting pieces of information in the text, such as people, locations, dates, crime-related words or email addresses.
Nowadays, one can find numerous services providing extraction of many different features.
We have chosen some of them and have collected results from testing on our mock legal case.
We compared results of the NLP services and a survey, in which participants labeled features from the text in a similar way as an NLP tool.

It is important to note that the field of extraction NLP features is a well-studied topic and the tools produce very stable results, probably using rather similar algorithms (which are described in the following section). Even though we could dedicate part of this work to developing well-suited algorithms for NLP feature extraction, we decided to take advantage of the existing tools and put the focus on processing and application of the features to the high-quality data mining and machine learning algorithms. 

\section{NLP Algorithms}
First, let us introduce the features and well-known algorithms for their extraction from a text in general.
Note that the tools mentioned further most likely use modified or even different techniques for extraction of the features.

\subsection{Keywords}
\label{algorithms_keywords}
\emph{Keywords} are people, places, words, ideas, which are understood as important in the given context. 
In our case, the context is the document, hence we expect the keyword to reflect what the document is really about.
Then, the \textit{keyness} is a quality measure of the importance the words have in the given text.
\cite[ch. 4]{scott2006textual}

We do not restrict the keyword to be one word because most of the tools are able to extract multi-word keywords, so-called \textit{key phrases}.
By combining words together, the phrases usually gain new meaning that cannot be inferred from the individual words.
Therefore, if we assumed only one-word keywords, we would never find the keyness hidden in the phrases.

The process of identifying keywords is driven by two factors.
First, the more often a word occurs in the document, the more likely it is a keyword.
And second, the more often a word occurs generally in a speech, the less likely it is a keyword of any document.
\cite[ch. 4]{scott2006textual}
The second factor ensures that words we use very often in the speech, such as prepositions, conjunctions or the most common nouns, are not considered as keywords, even though they occur in the document often.

Common implementation of the idea is called the \textit{TF-IDF} algorithm \cite[ch. 6]{manning2008introduction}.
The weight of a term is defined by its \textit{term frequency} and \textit{inverse document frequency}.
The term frequency tf$_{t,d}$ is simply a number of occurrences of the term $t$ in the document $d$. The \textit{inverse document frequency} is defined as
$$
\text{idf}_t = \log{\frac{N}{\text{df}_t}}
$$
where $N$ is the total number of documents and df$_{t}$ is a \textit{document frequency} and is equal to the number of documents, in which the term $t$ occurs.
It is important to take a logarithm of the expression because the \textit{inverted document frequency} can become very large with a large collection of documents and we need it to be comparable with the \textit{term frequency}, which is rather small.

The tf-idf weight is computed as follows \cite[ch. 6]{manning2008introduction}:
$$
\text{tf-idf}_{t, d} = \text{tf}_{t,d} \times \text{idf}_t
$$
The \textit{term frequency} ensures higher weight for more frequent terms in the document and \textit{inverse document frequency} ensures higher weight for more obscure words. Now tf-idf$_{t, d}$ can be a good approximation of our keyness measure and is often used as the keyword extraction technique. The last step is to establish a threshold value that distinguishes keywords and non-keywords by comparing it to the tf-idf values of the terms.

\subsection{Named Entity Recognition}
To extract valuable information from the text, we are interested in detecting named entities.
Those are entities that can be referred to with a proper name \cite[ch. 21]{jurafsky2014speech}.
In the speech, they are always noun phrases representing people, places, things, organizations, temporal or numerical expressions, events etc.

Lets pinpoint how important the named entities are in the context of legal cases.
Consider a typical use case of our application: a lawyer is searching through hundreds of emails with different senders and receivers.
The name of the only victim, place, and date of the crime are known.
If the lawyer is forced to manually search through the documents, he will filter the emails by occurrences of the aforementioned facts in the text.
All of the facts are named entities, which can be extracted from each document and eventually lead the algorithm to increase the relevance of documents accordingly.
Thanks to the named entities, the algorithm is performing the ranking in the same manner as the lawyer would do, which is our goal.

The academic approach to a find \textit{named entity} is often based on the statistical sequence model \cite[ch. 21]{jurafsky2014speech}.
Entity type and boundaries are found with one pass over the text according to a set of rules.
The rules include \textit{Part-of-Speech Tagging} 
\footnote{Detailed information about POS Tagging can be found in chapter 10 of \citetitle{jurafsky2014speech} by \citeauthor{jurafsky2014speech} \cite{jurafsky2014speech}}
, which helps to find the noun phrases, and therefore, the boundaries.
Next, a \textit{word shape} is examined, which represents a group of words by a general string representation. 
For instance, replacing any letter for X and any digit for d, we get a unique word shape dd XX for representing hours of the day written in US format, such as 12 am or 03 PM. 
Other rules include prefix matching or dictionary of predefined named entities.

On the other hand, the commercial approach is based on the combination of updated dictionaries, rules and supervised machine learning \cite{chiticariu2013rule}.
In the word of Big Data, it has become easier to collect large dictionaries of given names, family names, organizations or geological places (gazetteers).
The named entity recognition is performed by multiple passes over the text data.
First, the high-precision but low-recall rules are applied, then more entities are added by substring search of the previous entities.
In the next phase, the entities are consulted with the dictionaries and only then, the statistical sequence model is applied considering all the features from the previous stages \cite[ch. 21]{jurafsky2014speech}.

Further in the thesis and the application, we will work with the following set of \textit{named entities}:
\begin{itemize}
\item Person
\item Organization
\item Location
\item Time
\item Email Address
\item Crime
\item Health Condition
\end{itemize}

\subsection{Relations}
Once we have extracted the \textit{named entities}, we can start looking for the relationships between them.
As soon as we recognize entities and their relations, the main semantic meaning of a sentence is usually fully extracted.
In a legal environment, we can illustrate the added value of a relation by a testimony, in which a witness tells about both a person and a location, however, we are interested in the document only if there is a relation between these two, particularly was in or similar.

Lets see how the relations are represented.
Most of the \textit{relations extraction} algorithms are focused on binary relations only.
Multi-entity relations would be very expensive to extract.
We will borrow the representation from the Resource Description Framework (RDF) \cite{lassila1999resource} and define the relation as a triple
$$
\text{<subject, predicate, object>}
$$
where the \textit{subject} is an entity that performs an activity specified in the \textit{predicate} with respect to the \textit{object}.
The \textit{predicate} is the relation for us, it can be for example employed by, built, son of or cheated on.

The following groups of algorithms can be applied to extract the relations: \textbf{hand-written patterns}, \textbf{supervised machine learning}, \textbf{semi-supervised}, and \textbf{unsupervised}.
We will introduce the first two
\footnote{For more detailed description, see \citetitle{jurafsky2014speech} \cite{jurafsky2014speech}.}.

\subsubsection{\textbf{Hand-written patterns}}
In the work \citetitle{hearst1992automatic} by \citeauthor{hearst1992automatic} \cite{hearst1992automatic} we find the first idea of using patterns to extract relations.
The entity names are ignored and a relation is recognized by a set of patterns.
The patterns are based on the lexicology (POS tagging), the syntax (syntax tree) and the type of the entity (person, location, etc.).
For example, the pattern \cite[sec. 21.2]{jurafsky2014speech}
$$
NP_0\text{ such as }NP_1 \{, NP_2 ..., (\text{and|or})NP_i \}, i \geq 1
$$
is able to catch the relation hyponym for many occurrences, for instance, a sentence
$$
\text{... by any poison, such as Cyanide, Arsenic or Ricin ...}
$$
would generate relations 
\begin{itemize}
\item <poison, hyponym of, Cyanide>
\item <poison, hyponym of, Arsenic>
\item <poison, hyponym of, Ricin>
\end{itemize}
An example of a pattern using entity type \cite[sec. 21.2]{jurafsky2014speech}
$$
\text{\textbf{PERSON} (named|appointed|chose|...) \textbf{PERSON} Prep? \textbf{POSITION}}
$$
will determine a relation named. 

The big disadvantage of the pattern-based recognition is that the patterns need to be written by hand, which takes a lot of work. On the other hand, the precision of such method is usually high since the patterns are directed by a human.

\subsubsection{\textbf{Supervised machine learning}}
The second approach is to annotate a corpus of relations and entities and consider it as a training set in the machine learning.
More precisely, we need to extract features from the sentences, which serve as an input into the ML algorithm and the relations represents the labels for each sentence
\cite[sec 21.2]{jurafsky2014speech}.

The features can consist of unigrams or bigrams of the words in the sentence, especially between the subject and object. Next, the named entity types, POS tags or features from the syntax tree can be included as features. We can also consider the number of words between the subject and object or stemmed version of the words \cite[sec. 21.2]{jurafsky2014speech}.

The algorithm uses two models for the relations extraction.
The first one decides whether there exists a relation between two given entities, and the second one recognizes its type. Then, the algorithm first finds all pairs of entities in the sentence, test it with the first model, and only the positive results are tested on type with the second model \cite[sec. 21.2]{jurafsky2014speech}.

The supervised method can be very accurate on general inputs, however, to be able to do that, a large training dataset is required.
Therefore, the \textit{Distant Supervision for Relation Extraction} can be applied \cite{mintz2009distant}.
It is an elegant way, how to obtain a large training data.
The DBpedia \footnote{Comprehensive database of structured facts extracted from Wikipedia: \mbox{\url{http://wiki.dbpedia.org/}}} or 
Google Knowledge Graph \footnote{Database of world knowledge structured in a graph: \url{https://www.google.com/intl/bn/insidesearch/features/search/knowledge.html}}
provide enough information about some of the relations between real-world objects.
When we search a page about the subject on Wikipedia and find the sentence with the object, then we are ready to adopt the text as an input data for our training phase.
This approach delivers a lot of diverse sentences, in which we are confident about the relation type, and thus the training dataset is rich enough to create a reliable model.

\subsection{Topic Modeling}
\label{topic_modeling}
The \textit{Topic modeling} is a statistical approach for finding general topics that appear in the document.
The keywords are already able to discover some trends of a document, however, unlike the topics, they have to appear in the text.

Prior to the topic recognition, a fixed set of topics have to be defined.
The usual process is to assign probabilities of all topics to every word in the dictionary.
Then, if we consider a document as a bag-of-words
\footnote{Representation of a document that forgets ordering and counts of the words and keeps them in a simple set.}, 
we can compute the overall probability that a document has a specific topic only from its words.

One of the well-known algorithms for the topic modeling is called the \textit{Latent Dirichlet allocation} developed by \citeauthor{blei2003latent} in \citeyear{blei2003latent} \cite{blei2003latent}.
The model assumes that every document has fixed distribution of topics it belongs to.
Lets further assume that the words of a document were withdrawn from a specific distribution defined in the following way.
\begin{enumerate}
\item Pick a topic distribution of each document according to a \textit{Dirichlet distribution}.
\item Pick a word distribution for each topic.
\item For all words in all documents, generate the word as follows:
\begin{enumerate}
  \item Pick a random topic according to the distribution of topics for the document.
  \item Pick a random word from the distribution of words for the topic chosen in the step a).
\end{enumerate}
\end{enumerate}
The generation scheme tells us the probability theory for documents and topics, however, it does not tell us how to recognize the topics from the fixed documents.
To apply the theory, we need to reverse the process and assume that the documents were created by this generation process, and eventually guess the topic.
One of the iterative methods, which achieves this goal, is called the \textit{Collapsed Gibbs Sampling} \cite{xiao2010efficient}.
Put simply, the initialization phase picks a random topic for each word in all documents and each next phase goes over every word and changes its topic by computing the probabilities, assuming that all the other words are correctly labeled.



\subsection{Document Category}
There are two types of categories we are interested in.
The first is more related to the structure of the text and tells us where the document is coming from. 
Such types can be for instance email, medical record, police record, affidavit, etc.
The Legato system is prepared to work with such categories, and therefore we rely on the input to contain the information about the first type.

The second type is related to the meaning of the text.
Semantically, it is very similar to the topics introduced in the section \ref{topic_modeling}, and the only difference is that we need the most dominant topic to be chosen as a representative.

\section{Introduction of the tools}

\subsection{NLTK: The Natural Language Toolkit}
As a baseline for the commercial NLP tools, we included the open source python framework \textit{NLTK} \cite{bird2004nltk}.
Although it was originally developed for educational purposes, it is now broadly used by students, researchers and public to process the textual data in python.

\textit{NLTK} consists of multiple modules, which can be used as a pipeline during the text processing. 
The basic modules run the tokenizer, POS tagging, and syntactic analysis.
More advanced modules are dealing with the information extraction and knowledge representation.
The framework comes with large preprocessed corpora that can be used as reference corpora during the keyword or NE extraction \cite{bird2009natural}.

The commercial NLP tools do not need any settings since they are already configured and the configuration is basically the key to their success.
Nevertheless, the \textit{NLTK} is more a framework than a tool and we have to take the time to set the variables right.
It also brings a great advantage of highly configurable settings.
In the next two paragraphs, we explain our approach to extracting the keywords and entities in the \textit{NLTK}.

We applied the \textit{TF-IDF} algorithm to compute the keywords, as explained in \ref{algorithms_keywords}.
Firstly, we removed the stop words and filtered only meaningful words.
The list of stop words was used as defined by the \textit{Scikit-learn library} \footnote{Machine learning library for python \cite{pedregosa2011scikit}}.
To remove the nonsense words, we compared them against a dictionary of the \textit{Brown corpus}, which is the first text corpus of American English of about 1 million words \cite{brownCorpus}.
Next, we filtered only nouns and finally, computed the \textit{TF-IDF} score against the \textit{Reuters} reference corpus \cite[sec. 2.1]{bird2009natural}, which contains more than 10,000 news articles with 1.3 million words.
We have chosen the journalistic context because it was the closest domain to the legal environment.
This procedure outputs a score for each word in a document and the final keywords are those, which \textit{TF-IDF} score was more than a threshold of 0.15.

The entity extraction requires fewer settings since the NE tagger is already trained by the library \cite[sec. 7.5]{bird2009natural}.
We split the text into sentences and consequently sentences into tokens \footnote{token = word as a string without a disambiguated meaning}.
Next step is to extract the part-of-speech tags for each token.
The tokens together with the \textit{POS tags} are the input data into the \textit{NE chunker}, which is able to mark words as named entities and also determine its type.


\subsection{Google Cloud Natural Language}
Google has a long-standing experience with the text processing, especially in the field of information retrieval.
They decided to provide their knowledge of semantic text analysis in a service called Cloud Natural Language \cite{googleNLP}.
Google makes no secret of their approach: the same Deep Learning models that power the Google Search are also employed in the NLP service.

The tool is able to recognize entities, syntactic structure of sentences, category and sentiment of the text.
The sentiment analysis is recently very interesting topic because companies want to know public opinion of themselves.
However, the sentiment in legal documents is usually neutral and does not play a key role in the relevance classification, therefore we decided not to consider sentiment as a feature.

The \textit{Cloud Natural Language} tool also adds a level of confidence to every entity and category so that the user see how strong each feature is. Moreover, the tool replies with a Wikipedia article for every entity, if exists. Another useful feature is the Translation API, which converts text among many different languages and is able to recognize sources of an unknown language. Worth noting that the input does not have to be a pure text but also speech in the audio format or text in a picture.

\subsection{Watson Natural Language Understanding}
In March 2017, the IBM announced retirement of the \textit{AlchemyAPI} service, which was designed to understand the semantics of text and image by advanced techniques \cite{alchemyRetirement}.
One component of the service was also the \textit{AlchemyLanguage}, providing all NLP features, such as entity, sentiment and topic recognition.
As a replacement, a new service emerged: the \textit{Natural Language Understanding} service \cite{watsonNLP}.

Besides typical NLP features, such as keywords, entities, relations, category, sentiment or topics (which are called concepts in the \textit{NLU}), the service provides emotion recognition (joy, anger, fear, etc.) and metadata recognition.
The latter includes basic information about the author, title, prominent page image, and publication date.
Unfortunately, this feature is available only for HTML pages, which is usually not the type of our documents, hence we will not take advantage of the metadata recognition.

One of the most promising features of the \textit{NLU} service is the model customization.
All the NLP features are always recognized with respect to some general language model, which consists of words from all aspects and domains of our language.
This might be a disadvantage, especially when our documents are actually always of one type and contains words from a specific domain.
For instance, the general model would always consider law, justice or  sentence as keywords in our legal documents, however, they might appear in most of the documents and become not relevant anymore in the legal domain.
The \textit{NLU} service enables a user to create its own language model and load it into the cloud.
More than one model can be active and the user can switch between them before each query.

\subsection{Aylien}
\textit{Aylien} is a software package of information retrieval, machine learning, and natural language processing \cite{aylienNLP}, which provides competitive features to the mentioned NLP services from Google and IBM.

In addition to the classical NLP features, \textit{Aylien} provides summarization, which essentially picks the most important sentences from the text. In fact, this feature makes a good sense in our legal domain. Next innovative feature looks for related phrases, which are semantically as close to the original phrase as possible. Another advantage of \textit{Aylien} is that it includes time-based entities in the named entity recognition, specifically, dates. The previously mentioned services surprisingly ignore the time and date expressions most of the time.

Next interesting feature of \textit{Aylien} is the hashtag suggestion, however, it is basically the topic modeling as we described it in \ref{topic_modeling}.
In the same manner, the article extraction is another term for the IBMs metadata recognition.
Truly new feature is the aspect-based sentiment analysis, in which the sentiment is recognized for each aspect, not generally for the whole text.
For example, an analysis of hotel reviews can discover angry opinions on Wi-Fi signal but an excellent rating of the staff.

\section{Comparison}
To compare the tools, we had to restrict the evaluation set only to the common features included in all the tools.
Specifically, the \textit{keywords} and \textit{named entities}: persons, organizations, and locations.
Without any comparison, we also evaluated the relations, which were supported only by \textit{Watson NLU}, and dates supported only by \textit{Aylien}.

\subsection{Testing data: Legal mock case}
Of course, we could not use any real-world case for our testing and analysis due to privacy reasons, therefore, we analyzed a legal mock case called \textit{Davis v. HappyLand Toy Company}\cite{american2015davis} created by the \textit{American Mock Trial Association}\footnote{http://www.collegemocktrial.org/}.

Briefly described, the case puts a father of a child, who was killed by swallowed part of a toy, against the company that produced the dangerous toy. For our use case, we picked five most interesting and distinct documents: 
\begin{enumerate}
\item An email of an inter-company communication about the substances used in the toy.
\item Affidavit (testimony) of the father.
\item Affidavit of the babysitter, who looked after the child at the moment of the fatality.
\item Medical record from the autopsy of the child.
\item Journal paper about the origin, behavior, look, and impacts on a human body of the substance used in the toy.
\end{enumerate}

\subsection{Survey}
The survey was created in a way that no knowledge of the law is necessary, the questions are simple and clearly stated.
Some of the documents are several pages long, hence the survey can be saved and finished later.
The respondents were not provided with any extra information about the legal case so that the results are comparable with the NLP tools, which have no information as well.
The survey was completed by five developers of the Legato system.

We asked the same set of questions for every document.
First two questions asked about the keywords.
We have split the one-word phrases and multi-word phrases and required both with at least 10 one-word keywords.
Next, the respondent has to type people, organizations, and locations that appear in the text.
Moreover, we require relationships between people and dates together with a label specifying which event is related to the date.
Next two questions were related to the semantics and were not used to evaluate the tools.
We asked the respondents to copy a short passage from the text that could be used as an evidence at the court.
The second question asks for a short summary of the whole document.
Both of the questions will be later applied in the training and evaluation of the models
for document relevancy.


\subsection{Statistical comparison}

\begin{figure}
\centering
\caption{Jaccard index of the NLP tools evaluated against the survey with strict string comparison.}
\label{fig:jaccard_table_strict}
\footnotesize
\begin{tabular}{|l||l||*{5}{c|}}\hline
\rowcolor{orange!50}
\makebox[4em]{\textbf{Feature}}&\backslashbox{\textbf{Document}}{\textbf{Tool}}
&\makebox[3em]{\textbf{NLTK}}&\makebox[3em]{\textbf{Google}}
&\makebox[3em]{\textbf{Watson}}&\makebox[3em]{\textbf{Aylien}}\\\hline\hline
Keywords & Miller affidavit &0.11&&0.11&0.02\\\hline
& Davis affidavit &0.06&&0.03&0.03\\\hline
& Email &0.05&&0.24&0.20\\\hline
& Toxicology paper &0.00&&0.02&0.03\\\hline
& Medical record &0.09&&0.04&0.05\\\hline\hline
& \textbf{Average} &\textbf{0.06}&&\textbf{0.09}&\textbf{0.07} \\\hline\hline
People & Miller affidavit &0.00&0.29&0.54&0.38\\\hline
& Davis affidavit &0.00&0.13&0.25&0.24\\\hline
& Email &0.29&0.44&1.00&0.43\\\hline
& Toxicology paper &0.00&0.00&0.29&0.00\\\hline
& Medical record &0.00&0.27&0.71&0.57\\\hline\hline
& \textbf{Average} &\textbf{0.06}&\textbf{0.23}&\textbf{0.56}&\textbf{0.32}\\\hline\hline
Organizations & Miller affidavit &0.07&0.40&0.50&0.29\\\hline
& Davis affidavit &0.06&0.17&0.33&0.17\\\hline
& Email &0.00&0.25&0.50&0.00\\\hline
& Toxicology paper &0.00&0.33&0.33&0.14\\\hline
& Medical record &0.00&0.00&0.00&0.00\\\hline\hline
& \textbf{Average} &\textbf{0.03}&\textbf{0.23}&\textbf{0.33}&\textbf{0.12}\\\hline\hline
Locations & Miller affidavit &0.00&0.06&0.00&0.00\\\hline
& Davis affidavit &0.00&0.00&0.00&0.00\\\hline
& Email &0.00&0.00&0.00&0.00\\\hline
& Toxicology paper &0.00&0.00&0.00&0.00\\\hline
& Medical record &0.14&0.00&0.00&0.00\\\hline\hline
& \textbf{Average} &\textbf{0.03}&\textbf{0.01}&\textbf{0.00}&\textbf{0.00}\\\hline\hline
\end{tabular}
\normalsize
\end{figure}

\begin{figure}
\centering
\caption{Jaccard index of the NLP tools evaluated against the survey with subset tolerance in string comparison.}
\label{fig:jaccard_table_subset}
\footnotesize
\begin{tabular}{|l||l||*{5}{c|}}\hline
\rowcolor{orange!50}
\makebox[4em]{\textbf{Feature}}&\backslashbox{\textbf{Document}}{\textbf{Tool}}
&\makebox[3em]{\textbf{NLTK}}&\makebox[3em]{\textbf{Google}}
&\makebox[3em]{\textbf{Watson}}&\makebox[3em]{\textbf{Aylien}}\\\hline\hline
Keywords & Miller affidavit &0.30&&0.47&0.57\\\hline
& Davis affidavit &0.35&&0.26&0.33\\\hline
& Email &0.14&&0.57&0.50\\\hline
& Toxicology paper &0.09&&0.37&0.25\\\hline
& Medical record &0.23&&0.24&0.15\\\hline\hline
& \textbf{Average} &\textbf{0.22}&&\textbf{0.38}&\textbf{0.36} \\\hline\hline
People & Miller affidavit &0.79&0.32&0.85&0.94\\\hline
& Davis affidavit &0.64&0.29&0.63&0.47\\\hline
& Email &1.00&0.56&1.00&0.71\\\hline
& Toxicology paper &0.00&0.06&0.29&0.29\\\hline
& Medical record &0.57&0.40&0.71&0.83\\\hline\hline
& \textbf{Average} &\textbf{0.60}&\textbf{0.32}&\textbf{0.69}&\textbf{0.65}\\\hline\hline
Organizations & Miller affidavit &0.36&0.40&0.50&0.71\\\hline
& Davis affidavit &0.06&0.17&0.33&0.17\\\hline
& Email &0.50&0.50&1.00&0.50\\\hline
& Toxicology paper &0.29&0.33&0.33&0.29\\\hline
& Medical record &0.16&0.00&0.00&0.00\\\hline\hline
& \textbf{Average} &\textbf{0.27}&\textbf{0.28}&\textbf{0.43}&\textbf{0.33}\\\hline\hline
Locations & Miller affidavit &0.27&0.61&0.67&0.29\\\hline
& Davis affidavit &0.00&0.10&0.00&0.00\\\hline
& Email &0.00&0.00&0.00&0.00\\\hline
& Toxicology paper &0.00&0.00&0.00&0.00\\\hline
& Medical record &0.14&0.00&0.00&0.00\\\hline\hline
& \textbf{Average} &\textbf{0.08}&\textbf{0.14}&\textbf{0.13}&\textbf{0.06}\\\hline\hline
\end{tabular}
\normalsize
\end{figure}

Since the survey was completed by only five respondents, we decided to unify results for each answer.
The standard approach how to compute agreement between two sources is the \textit{Cohens kappa measure} \cite{smeeton1985kappa}.
The problem with \textit{kappa measure} is that the sources assign a fixed number of items to classes, which is not our situation.
In our case, the number of items is variable and we do not have any classes.
We care about the agreement between the sets.
Therefore, we are using a measure for the distance between sets, the \textit{Jaccard index} \cite{levandowsky1971distance}, defined as follows:
$$
J(A, B) = \frac{|A \cap B|}{|A \cup B|}
$$
Since the union is always larger than or equal to the intersection, we get $0 \leq J(A, B) \leq 1$ for any sets $A$, $B$. 
When $J(A,B)=1$, the sets are identical and when $J(A,B)=0$, the sets do not share any item.

The next step is to define when two items (keywords and entities) are equal.
We have defined two approaches in comparing the equality: \textit{strict approach}, and \textit{subset tolerance approach}.

In the \textit{strict approach}, all items are converted to lower case, so that the names, organizations, locations and other items are compared regardless of the capital letters.
Apart from that, no changes are made to the items and the comparison is based on a strict string equality.
As a consequence, for instance, full name and given name alone are not equal, even if the text represents the same person. 
E.g. for sets $A=\{\text{"Joey Davis", "Brett"}\}$ and $B=\{\text{"Joey", "Brett"}\}$ the Jaccard index is 
$$
J(A, B) = \frac{|\{\text{"Brett"}\}|}{|\{\text{"Joey Davis", "Joey", "Brett"}\}|} = 1/3
$$
Figure \ref{fig:jaccard_table_strict} shows \textit{Jaccard indices} using the strict approach for each combination of tool, feature and document compared with the ground truth, which is the survey output.
The "\textbf{Average}" row displays the mean over all documents for each tool and feature.

The \textit{subset tolerance approach} converts the items to lower case in the same manner as the \textit{strict approach}.
Then, two items are equal if one is a subset of the other or they are strictly equal.
To keep the construction of unions and intersections simple, we include both superset and subset in the union as well as in the intersection.
E.g. for sets $A=\{\text{"Joey Davis", "Brett"}\}$ and $B=\{\text{"Joey", "Davis"}\}$ the Jaccard index is 
$$
J(A, B) = \frac{|\{\text{"Joey Davis", "Joey", "Davis"}\}|}{|\{\text{"Joey Davis", "Joey", "Brett", "Davis"}\}|} = 3/4
$$
Results of the \textit{subset tolerance approach} are displayed in the figure \ref{fig:jaccard_table_subset} in the same form as in \ref{fig:jaccard_table_strict}.
One can notice that the score is always equal or higher compared to the \textit{strict approach}, which is a natural consequence of the approach definitions.

\subsubsection{\textbf{Keywords}}
All tools except for the \textit{Google Cloud Natural Language} service provide the keywords extraction.
The \textit{Jaccard index} is generally very low in the case of the keywords.
There is usually a lot of words to choose from and even the respondents agreed only on few of them.
We can notice the best scores in the case of the email due to the short length of the text.
The \textit{Watson NLU} achieved the best score, however, it did not perform much better than our baseline \textit{NLTK}.
\textit{Aylien} scores high in the \textit{subset tolerance approach} because it extracts very long keywords, which have a high probability of being superset of some of the real keyword.

\subsubsection{\textbf{People}}
The scores for the people extraction were surprisingly high.
In the \textit{strict approach}, the \textit{NLTK} failed because of its limitation of recognizing only one-word items.
People names mostly appeared with both given and family name and the \textit{NLTK} recognized the name separately as two items.
On the other hand, when the subsets are tolerated, the \textit{NLTK} achieved very good result.
The winner of this category is the \textit{Watson NLU}, which is superior for every document and approach.
\textit{Aylien} service achieved a better score than \textit{Google CNL}, especially in the case of the longer documents.

\subsubsection{\textbf{Organizations}}
The \textit{NLTK} achieved the lowest score for the same reason as in the case of the people extraction since the names of organizations also consist of more than one word.
In the strict variant, all other tools performed significantly better than the baseline, yet the best score again achieved the \textit{Watson NLU} service.
In this category, \textit{Google CNL} outperformed the \textit{Aylien} service in the \textit{strict approach}, however, we can see reversed result in the \textit{subset tolerance approach}.
One of the possible explanations is the fact that \textit{Google} is able to precisely determine borders of the entities but they are not so often common with the survey output.
Whereas \textit{Aylien} usually fails at determining the borders since the organizations often consist of many words but the items contain the right information in the subsets.
The Medical record, unfortunately, does not contain many organizations, therefore none services were successful.

\subsubsection{\textbf{Locations}}
The extraction of the locations was quite chaotic and diverse by the respondents as well as by the tools.
The location can be understood as a named entity which is unique in the world, such as Hype park in London, 107 Myers street or Moscow, whereas someone understands the location as any expression specifying the environment, such as living room, school or a car.
The latter type was mostly recognized by the \textit{Google CNL} and the other tools recognized the first type.
Moreover, there were not enough location entities in the documents to draw a meaningful conclusion from the results.
In the strict variant, we ascribe win of the \textit{NLTK} service to a coincidence, as its simple model extracted many words starting with capitals, hence more people than locations appeared in the output set.
In the subset tolerance variant, \textit{Google CNL} and \textit{Watson NLU} achieved very similar score, however, the Watson generated approximately ten times fewer items, which brings us to a conclusion that the discriminative power of the Watsons items might be higher.

\subsubsection{\textbf{Dates}}
As the \textit{Aylien} service is the only one that is able to extract the time-based entities, we have not performed any comparison.
Still, we wanted to see how precise is the extraction, and so we present the \textit{Jaccard index} computed from the \textit{Aylien} output and the survey data.

\begin{figure}[h]
\centering
\caption{Jaccard index of the time-based entities extracted by Aylien.}
\label{fig:jaccard_dates}
\footnotesize
\begin{tabular}{|l||*{5}{c|}}\hline
\rowcolor{orange!50}
\makebox[4em]{\textbf{Document}}&{\textbf{Jaccard index}}\\\hline\hline
Millers affidavit & 0.07 \\\hline
Davis affidavit &0.13\\\hline
Email &0.00\\\hline
Toxicology paper &0.00\\\hline
Medical record &0.00\\\hline\hline
\textbf{Average} &\textbf{0.04}\\\hline
\end{tabular}
\normalsize
\end{figure}
The \textit{Aylien} achieved non-zero results only in the longer documents and even then, the scores are under our estimation.
As an example, a simple date in form "24. 12. 2017" is recognized as a phone rather than a date.
Here, it strongly depends on the spaces: after we remove a space before the month -- "24.12. 2017" -- the day and month are finally recognized as a date, however, the whole string is still reported as a phone.
On the other hand, we took into consideration only the dates, while \textit{Aylien} recognizes times and other expressions, such as "last week", as well.

\subsection{Analytical comparison}
Aside from the statistical evaluation of the tools accuracy, it is also important to manually analyze the output and check whether it is semantically correct.
In this section, we discuss characteristics of the tools and their benefits and drawbacks.

\subsubsection{\textbf{NLTK}}
We already outlined two drawbacks of the \textit{NLTK} in the previous section: one-word limitation and simple NE model.
The first problem is visible in both affidavits, where the toy name appears under the name "Princess Beads", which is an obvious keyword in this context.
Interestingly, the \textit{NLTK} extracted both parts: "princess" and "beads" as keywords since the words are quite unique by itself.
However, the key phrase was missed.
As the \textit{NLTK} is highly customizable, we believe that the multi-word phrases extraction is indeed possible but it would require an extra amount of work.

On the other hand, the keyword extraction performance was very well comparable to the commercial tools.
\textit{NLTK} performed considerably worse in the entity extraction.
Besides the separation of the given and family name, it also falsely marked "Liquid", "Myth" or "Identification" as people, probably because of the capital letters.
One can notice a lot of false positives in the Medical record, were a lot of words were written in the capitals and \textit{NLTK} recognized all of them as organizations, e.g. "BODY", "AND" or "OF".

Although the \textit{NLTK} provides a very good baseline for the NLP feature extraction, we can see a noticeable line between its hand-written patterns and the supervised machine learning models, which are used by the other tools.

\subsubsection{\textbf{Google Cloud Natural Language}}
The approach of the Google is a little different in the following way.
Together with the entity, it outputs also a position in the text and it does not discard duplicates.
\textit{Google} does not understand the extraction as picking of the most identifying words, but more as a highlighting of the words in the text.
Therefore, the \textit{Google CNL} usually produces a larger output than others.
The true positive rate is almost perfect, for example in the Millers affidavit, all of "Brett Miller", "Lee Davis", "Andy Davis", "Hillary Davis" and "Joey Davis" were recognized.
Even more surprising is that "Hillary Davis" appears only once in the text and \textit{Google} was able to link the name to many occurrences of her given name.
The problem is that words, such as "kids", "one", or "friend" are recognized as well, and truth is that they really represent some people in the meaning, however, it is not necessary and rather disturbing in our use case.

\subsubsection{\textbf{Watson Natural Language Understanding}}
The \textit{Watson} approach is slightly more moderate.
Very little false positives are introduced as \textit{Watson} outputs a word only in case of high confidence.
Similarly as \textit{Google}, \textit{Watson} recognized all people in the Millers affidavit including full name of "Hillary Davis".
Due to the conservativeness, only the family name of "Chase Tuchmont" was recognized since "chase" is an ordinary word in English as well.
However, the overwhelming majority of the recognized entities makes perfect sense and are truly people, organizations or locations.
Therefore, the \textit{Watson NLU} service is not only superior in the statistical evaluation, but also in the analytical overview of the semantics.

\subsubsection{\textbf{Aylien}}
We can see yet another approach by the \textit{Aylien} service.
Keywords are understood as complex pieces of information of arbitrary length, sometimes even more than 10 words long.
As a result, service correctly extracted "Joye" as a keyword in the Davis affidavit, though incorrectly extracted a key phrase "Davis and that I wanted to speak with someone about Princess", which seems rather awkward.
This was a source of a low score of \textit{Aylien} in the keywords category.

On the other hand, \textit{Aylien} excelled in the extraction of people.
Again, all people in the Millers affidavit were successfully recognized, even though the model was not sophisticated enough to merge "Hillary" and "Hillary Davis".
Despite a few mistakes, mostly recognizing numbers as people, the service was able to keep high true positives and low false positives, including organizations and locations.


\chapter{Classification}
Data exploration in the chapter \ref{chp:data_characteristics} helped us clarify the language and style of the legal documents enough to design models for the relevance classification.
We present a pipeline of three machine learning models that are consecutively making decisions about the documents to output the final relevance.
First is a \textit{Law-Case classifier} \ref{sec:law-case-classifier}, the second is a \textit{Sentence classifier} \ref{sec:sentence-classifier} and the last is a \textit{Document classifier} \ref{sec:document-classifier}.
Each of them needs a different set of attributes as an input and applies a suitable machine learning model.
For further details about the individual classifiers, see the related sections.

\section{Law-Case classifier}
\label{sec:law-case-classifier}

\section{Sentence relevance classifier}
\label{sec:sentence-classifier}

\subsection{Attributes}
To achieve a successful classification, one of the most crucial steps is to convert the data into suitable attributes, which will be most helpful in determining the classes.
Technically, all machine learning models are built to process numbers as input variables.
Since we work here with text and only text, we have to find a way how to convert all keywords, entities, relations, syntactic structure, and other features into numerical representation.
During this process, we have to keep in mind the main purpose: the numbers have to be hiding information about the relevance of the sentence.

\subsubsection{\textbf{Sentence meaning extraction}}
When extracting a meaning of a sentence, one will encounter two major problems to be solved: 1) how to represent meaning of a single word 2) how to combine word meanings.

The first problem is also known as the \textit{Word Embedding} problem \cite{wordEmbeddings}.
The simplest solution is to assign unique numbers to all words and train classifiers on the numbers instead of words.
$$
\{\text{"knife"} \rightarrow 1, \text{"gun"} \rightarrow 2, \text{"wire"} \rightarrow 3, \dots\}
$$
This approach does not work since the classifiers interpret a number as a quantitative, not nominative,  attribute.
As a result, the classifier understands two words with close numbers as close by the meaning as well.
Since the numbers were assigned without any order, there is undoubtedly a lot of misinterpretation.

The next solution is often applied in the information retrieval systems.
Instead of one number, we can represent the word embedding by a vector, with all elements zeroed but one.
Each word is assigned an unique index, on which the vector keeps a value of one.
$$
\{\text{"knife"} \rightarrow (1,0,0), \text{"gun"} \rightarrow (0,1,0), \text{"wire"} \rightarrow (0,0,1), \dots\}
$$
Now, a vector uniquely identifies a word and the vectors are not comparable for the classifier, which eliminates the problem with the previous approach.
This solution can be further improved by counting the words in the training documents, TF-IDF weighting or co-occurrence matrices \cite{wordEmbeddings}.

The third and final solution goes even further in the embedding of the semantics.
Recent research by Mikolov et al. \cite{mikolov2013efficient} introduced a revolutionary algorithm \textit{Word2vec}, which uses a neural network to create a vectors of fixed length with its semantics compressed.
Each dimension of a vector represents some concept, which can be somehow quantified, such as masculinity, age or liveliness \cite{mikolov2013linguistic}.
The vectors are capturing semantic regularities, and therefore, the classifiers have a potential to perform better even for unknown words.
This embedding ensures our ultimate assumption, that close vectors are also close by the meaning of their underlying words.

Both the algorithm and the trained vectors are publicly available.
Though we could use the algorithm to train our own vectors, it is much easier to adopt the already trained dataset.
Since the meaning of the words is the same regardless of the area, there would be a very little gain in the training a custom dataset specialized on legal documents.
The dictionary we will use is trained by the \textit{Google} company and is available on their official code archive \cite{word2vecGoogle}.
The pre-trained model contains 3 million unique words of 300-dimensional vectors, which was trained on a 300 billion word news dataset.
For our purpose, a trimmed dataset of 300 thousand unique words without the phrases is sufficient \footnote{\url{Downloaded from https://github.com/eyaler/word2vec-slim}}.

The second problem relates to the fact, that the classifiers usually need fixed input, but we currently computed vectors for each words of a sentence.
Researches Vedantam et al. \cite{vedantam2015learning} applied a simple average of \textit{word2vec} vectors to the words in a phrase to get the final embedding.
We can extend the idea to all sentences and achieve one vector per sentence by computing an average of all words in the sentence.
To achieve more interesting results, it is possible to weight the words by their TF-IDF score in the overall average, however, the contribution is not necessary positive since the ordinary words can distinguish relevant sentence as well as the keyword.
Besides, we embedded the keywords (words with high TF-IDF) in another representation, hence the TF-IDF weighting does not bring enough contribution to be worth the processing time.


\subsubsection{\textbf{Extracted features}}


\subsubsection{\textbf{Syntax structure}}



\section{Document relevance classifier}
\label{sec:document-classifier}
[Law-Document and Case-Document classifier]


\chapter{Implementation}
\section{Frameworks and libraries}
\section{Design and implementation}

\subsection{Text preprocessing}
The documents come into the Legato system in one of the two formats: a text in a digital form or an image.
The latter needs to be converted into the former, which is achieved by the Optical Character Recognition\footnote{recognition of text characters from a picture and conversion into a text in a digital form.} (OCR).
Unfortunately, the OCR technique is not perfect and leaves specific artifacts in the text.
Therefore, we first apply the text processing, which filters, edits and removes these artifacts and prepares the text to be segmented into sentences.

Firstly, all empty lines are removed and consecutive white spaces are converted into a single space character.
The structure of a document is lost with this edit, which might mean a loss of some information, however, we decided to focus on the semantics more than the structure.
Secondly, the OCR artifacts has to be removed.
For example, the recognition puts '\&\#39;' instead of the apostrophe and '\&quot;' instead of the quotation.
Another artifact of the OCR are coupled words with missing spaces -- a dictionary of English language is applied to break the words apart.
The final step is a removal of a hyphenation that was introduced to break new lines in the original document.

\section{Integration into Legato}

\chapter{Evaluation}
\section{Dataset introduction}
\section{Model selection}
\section{Parameter tuning}
\section{Results}

\chapter{Conclusion}



  \printbibliography[heading=bibintoc] %% Print the bibliography.

  \makeatletter\thesis@blocks@clear\makeatother
  \phantomsection %% Print the index and insert it into the
  \addcontentsline{toc}{chapter}{\indexname} %% table of contents.
  \printindex

\appendix %% Start the appendices.
\chapter{An appendix}
Here you can insert the appendices of your thesis.

\end{document}
