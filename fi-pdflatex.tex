%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this work into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  digital, %% This option enables the default options for the
           %% digital version of a document. Replace with `printed`
           %% to enable the default options for the printed version
           %% of a document.
  table,   %% Causes the coloring of tables. Replace with `notable`
           %% to restore plain tables.
  lof,     %% Prints the List of Figures. Replace with `nolof` to
           %% hide the List of Figures.
  lot,     %% Prints the List of Tables. Replace with `nolot` to
           %% hide the List of Tables.
  %% More options are listed in the user guide at
  %% <http://mirrors.ctan.org/macros/latex/contrib/fithesis/guide/mu/fi.pdf>.
]{fithesis3}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the T2A font encoding
\usepackage[T1,T2A]{fontenc}  %% to use the Cyrillic fonts with Russian texts.
\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
  english, german, russian, czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%
%%   \begin{otherlanguage}{german}  ... \end{otherlanguage}
%%   \begin{otherlanguage}{russian} ... \end{otherlanguage}
%%   \begin{otherlanguage}{czech}   ... \end{otherlanguage}
%%   \begin{otherlanguage}{slovak}  ... \end{otherlanguage}
%%
%% For non-Latin scripts, it may be necessary to load additional
%% fonts:
\usepackage{paratype}
\usepackage{textcomp} % to be able to use normal apostrophe with \textquotesingle
\def\textrussian#1{{\usefont{T2A}{PTSerif-TLF}{m}{rm}#1}}
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{
    date          = 2018/05/20,
    university    = mu,
    faculty       = fi,
    type          = mgr,
    author        = Jiří Mauritz,
    gender        = m,
    advisor       = Ing. Libor Kubečka, 
    title         = {Automatic Categorization of Legal Documents},
    TeXtitle      = {Automatic Categorization of Legal Documents},
    keywords      = {machine learning, legal documents, natural language processing},
    TeXkeywords   = {machine learning, legal documents, natural language processing},
    abstract      = {This is the abstract of my thesis, which can

                     span multiple paragraphs.},
    thanks        = {These are the acknowledgements for my thesis, which can

                     span multiple paragraphs.},
    bib           = example.bib,
}
\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.
%% These additional packages are used within the document:
\usepackage{paralist} %% Compact list environments
\usepackage{amsmath}  %% Mathematics
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}      %% Hyperlinks
\usepackage{markdown} %% Lightweight markup
\usepackage{listings} %% Source code highlighting
\lstset{
  basicstyle      = \ttfamily,%
  identifierstyle = \color{black},%
  keywordstyle    = \color{blue},%
  keywordstyle    = {[2]\color{cyan}},%
  keywordstyle    = {[3]\color{olive}},%
  stringstyle     = \color{teal},%
  commentstyle    = \itshape\color{magenta}}
\usepackage{floatrow} %% Putting captions above tables
\floatsetup[table]{capposition=top}
\begin{document}
\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

Theses are rumoured to be the capstones of education, so I decided
to write one of my own. If all goes well, I will soon have a
diploma under my belt. Wish me luck!

\chapter{Background}
\section{Legato system}
\section{Analysis of similar tools}
\chapter{Natural Language Processing: Feature extraction}
The amount of text in the legal documents can vary largely.
Most of the documents we are working with stretch across one page, however, the average number of pages is 6 and we can find documents up to hundred pages long.
The typical approach of the text categorization techniques is to create a document-term matrix containing counts of terms in each document.
The problem with this approach is that the long documents are represented by a vector, which \emph{information gain} is rather low.
For us, a high \emph{information gain} means that the representation captures the general idea, purpose of the document, and similarity/dissimilarity with other documents.

To address the issue, we decided to represent a document by specific features extracted by one of the NLP tools.
We are looking for potentially the most interesting information in the text, such as people, locations, dates, crime related words or email addresses.
Nowadays, one can find numerous services providing extraction of many different features.
We have chosen some of them and have collected results from testing on our mock legal case.
We evaluated results based on analysis of our use-case and a survey, in which participants labeled features from the text in a similar way as a NLP tool.

[odstavec o tom, proc pouzivam tooly a nedelam to sam]

\section{NLP Algorithms}
First, let us introduce the features and well-known algorithms for their extraction from a text in general.
Note that the tools mentioned further most likely use modified or even different techniques for extraction of the features.

\subsection{Keywords}
\emph{Keywords} are people, places, words, ideas, which are understood as important in the given context. 
In our case, the context is the document, hence we expect the keyword to reflect what the document is really about.
Then, the \textit{keyness} is a quality measure of the importance the words have in the given text.
\cite[ch. 4]{scott2006textual}

We do not restrict the keyword to be one word, because most of the tools are able to extract multi-word keywords, so called \textit{key phrases}.
By combining words together, the phrases usually gain new meaning that cannot be inferred from the individual words.
Therefore, if we assumed only one-word keywords, we would never find the keyness hidden in the phrases.

The process of identifying keywords is driven by two factors.
First, the more often a word occurs in the document, the more likely it is a keyword.
And second, the more often a word occurs generally in a speech, the less likely it is a keyword of any document.
\cite[ch. 4]{scott2006textual}
The second factor ensures that words we use very often in the speech, such as prepositions, conjuctions or the most common nouns, are not considered as keywords, even though they occur in the document often.

Common implementation of the idea is called the \textit{TF-IDF} algorithm \cite[ch. 6]{manning2008introduction}.
The weight of a term is defined by its \textit{term frequency} and \textit{inverse document frequency}.
The term frequency tf$_{t,d}$ is simply a number of occurrences of the term $t$ in the document $d$. The \textit{inverse document frequency} is defined as
$$
\text{idf}_t = \log{\frac{N}{\text{df}_t}}
$$
where $N$ is the total number of documents and df$_{t}$ is a \textit{document frequency} and is equal to the number of documents, in which the term $t$ occurs.
It is important to take a logarithm of the expression because the \textit{inverted document frequency} can become very large with large collection of documents and we need it to be comparable with the \textit{term frequency}, which is rather small.

The tf-idf weight is computed as follows \cite[ch. 6]{manning2008introduction}:
$$
\text{tf-idf}_{t, d} = \text{tf}_{t,d} \times \text{idf}_t
$$
The \textit{term frequency} ensures higher weight for more frequent terms in the document and \textit{inverse document frequency} ensures higher weight for more obscure words. Now tf-idf$_{t, d}$ can be a good approximation of our keyness measure and is often used as the keyword extraction technique. The last step is to establish a threshold value that distinguishes keywords and non-keywords by comparing it to the tf-idf values of the terms.

\subsection{Named Entity Recognition}
To extract valuable information from the text, we are interested in detecting named entities.
Those are entities that can be referred to with a proper name \cite[ch. 21]{jurafsky2014speech}.
In the speech, they are always noun phrases representing people, places, things, organizations, temporal or numerical expressions, events etc.

Let's pinpoint how important the named entities are in the context of legal cases.
Consider a typical use case of our application: a lawyer is searching through hundreds of emails with different senders and receivers.
The name of the only victim, place and date of the crime are known.
If the lawyer is forced to manually search through the documents, he will filter the emails by occurrences of the aforementioned facts in the text.
All of the facts are named entities, which can be extracted from each document and eventually lead the algorithm to increase the relevance of documents accordingly.
Thanks to the named entities, the algorithm is performing the ranking in the same manner as the lawyer would do, which is our goal.

The academic approach to a find \textit{named entity} is often based on the statistical sequence model \cite[ch. 21]{jurafsky2014speech}.
Entity type and boundaries are found with one pass over the text according to a set of rules.
The rules include \textit{Part-of-Speech Tagging} 
\footnote{Detailed information about POS Tagging can be found in chapter 10 of \citetitle{jurafsky2014speech} by \citeauthor{jurafsky2014speech} \cite{jurafsky2014speech}}
, which helps to find the noun phrases, and therefore, the boundaries.
Next, a \textit{word shape} is examined, which represents a group of words by a general string representation. 
For instance, replacing any letter for 'X' and any digit for 'd', we get a unique word shape 'dd XX' for representing hours of the day written in US format, such as '12 am' or '03 PM'. 
Other rules include prefix matching or dictionary of predefined named entities.

On the other hand, the commercial approach is based on combination of updated dictionaries, rules and supervised machine learning \cite{chiticariu2013rule}.
In the word of Big Data, it has become easier to collect large dictionaries of given names, family names, organizations or geological places (gazetteers).
The named entity recognition is performed by multi pass over the text data.
First, the high-precision but low-recall rules are applied, then more entities are added by substring search of the previous entities.
In the next phase, the entities are consulted with the dictionaries and only then, the statistical sequence model is applied considering all the features from the previous stages \cite[ch. 21]{jurafsky2014speech}.

Further in the thesis and the application, we will work with the following set of \textit{named entities}:
\begin{itemize}
\item Person
\item Organization
\item Location
\item Time
\item Email Address
\item Crime
\item Health Condition
\end{itemize}

\subsection{Relations}

\subsection{Document Category}

\subsection{Concepts}

\section{Introduction of the tools}

\subsection{Python Natural Language Toolkit}
Python provides a library for natural language processing \footnote{\url{http://www.nltk.org}}, which 
\subsection{Google Cloud Natural Language}
\subsection{Watson Natural Language Understanding}
\subsection{Aylien}

\section{Comparison}
[union of results from respondents, then kappa measure]
\subsection{Mock legal case}
\chapter{Classification}
\section{Attributes}
\section{Models}

\chapter{Implementation}
\section{Frameworks and libraries}
\section{Design and implementation}
\section{Integration into Legato}

\chapter{Evaluation}
\section{Dataset introduction}
\section{Model selection}
\section{Parameter tuning}
\section{Results}

\chapter{Conclusion}



  \printbibliography[heading=bibintoc] %% Print the bibliography.

  \makeatletter\thesis@blocks@clear\makeatother
  \phantomsection %% Print the index and insert it into the
  \addcontentsline{toc}{chapter}{\indexname} %% table of contents.
  \printindex

\appendix %% Start the appendices.
\chapter{An appendix}
Here you can insert the appendices of your thesis.

\end{document}
