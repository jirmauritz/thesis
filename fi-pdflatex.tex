%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this work into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  digital, %% This option enables the default options for the
           %% digital version of a document. Replace with `printed`
           %% to enable the default options for the printed version
           %% of a document.
  table,   %% Causes the coloring of tables. Replace with `notable`
           %% to restore plain tables.
  lof,     %% Prints the List of Figures. Replace with `nolof` to
           %% hide the List of Figures.
  lot,     %% Prints the List of Tables. Replace with `nolot` to
           %% hide the List of Tables.
  %% More options are listed in the user guide at
  %% <http://mirrors.ctan.org/macros/latex/contrib/fithesis/guide/mu/fi.pdf>.
]{fithesis3}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the T2A font encoding
\usepackage[T1,T2A]{fontenc}  %% to use the Cyrillic fonts with Russian texts.
\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
  english, german, russian, czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%
%%   \begin{otherlanguage}{german}  ... \end{otherlanguage}
%%   \begin{otherlanguage}{russian} ... \end{otherlanguage}
%%   \begin{otherlanguage}{czech}   ... \end{otherlanguage}
%%   \begin{otherlanguage}{slovak}  ... \end{otherlanguage}
%%
%% For non-Latin scripts, it may be necessary to load additional
%% fonts:
\usepackage{paratype}
\usepackage{textcomp} % to be able to use normal apostrophe with \textquotesingle
\def\textrussian#1{{\usefont{T2A}{PTSerif-TLF}{m}{rm}#1}}
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{
    date          = 2018/05/20,
    university    = mu,
    faculty       = fi,
    type          = mgr,
    author        = Jiří Mauritz,
    gender        = m,
    advisor       = Ing. Libor Kubečka, 
    title         = {Automatic Categorization of Legal Documents},
    TeXtitle      = {Automatic Categorization of Legal Documents},
    keywords      = {machine learning, legal documents, natural language processing},
    TeXkeywords   = {machine learning, legal documents, natural language processing},
    abstract      = {This is the abstract of my thesis, which can

                     span multiple paragraphs.},
    thanks        = {These are the acknowledgements for my thesis, which can

                     span multiple paragraphs.},
    bib           = example.bib,
}
\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.
%% These additional packages are used within the document:
\usepackage{paralist} %% Compact list environments
\usepackage{amsmath}  %% Mathematics
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}      %% Hyperlinks
\usepackage{markdown} %% Lightweight markup
\usepackage{listings} %% Source code highlighting
\lstset{
  basicstyle      = \ttfamily,%
  identifierstyle = \color{black},%
  keywordstyle    = \color{blue},%
  keywordstyle    = {[2]\color{cyan}},%
  keywordstyle    = {[3]\color{olive}},%
  stringstyle     = \color{teal},%
  commentstyle    = \itshape\color{magenta}}
\usepackage{floatrow} %% Putting captions above tables
\floatsetup[table]{capposition=top}
\begin{document}
\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

Theses are rumoured to be the capstones of education, so I decided
to write one of my own. If all goes well, I will soon have a
diploma under my belt. Wish me luck!

\chapter{Background}
\section{Legato system}
\section{Analysis of similar tools}
\chapter{Natural Language Processing: Feature extraction}
The amount of text in the legal documents can vary largely.
Most of the documents we are working with stretch across one page, however, the average number of pages is 6 and we can find documents up to hundred pages long.
The typical approach of the text categorization techniques is to create a document-term matrix containing counts of terms in each document.
The problem with this approach is that the long documents are represented by a vector, which \emph{information gain} is rather low.
For us, a high \emph{information gain} means that the representation captures the general idea, purpose of the document, and similarity/dissimilarity with other documents.

To address the issue, we decided to represent a document by specific features extracted by one or more NLP tools.
We are looking for potentially the most interesting pieces of information in the text, such as people, locations, dates, crime related words or email addresses.
Nowadays, one can find numerous services providing extraction of many different features.
We have chosen some of them and have collected results from testing on our mock legal case.
We compared results of the NLP services and a survey, in which participants labeled features from the text in a similar way as a NLP tool.

It is important to note that the field of extraction NLP features is well-studied topic and the tools produce very stable results, probably using rather similar algorithms (which are described in the following section). Even though we could dedicate part of this work to developing well-suited algorithms for NLP feature extraction, we decided to take advantage of the existing tools and put the focus on processing and application of the features to the high-quality data mining and machine learning algorithms. 

\section{NLP Algorithms}
First, let us introduce the features and well-known algorithms for their extraction from a text in general.
Note that the tools mentioned further most likely use modified or even different techniques for extraction of the features.

\subsection{Keywords}
\emph{Keywords} are people, places, words, ideas, which are understood as important in the given context. 
In our case, the context is the document, hence we expect the keyword to reflect what the document is really about.
Then, the \textit{keyness} is a quality measure of the importance the words have in the given text.
\cite[ch. 4]{scott2006textual}

We do not restrict the keyword to be one word, because most of the tools are able to extract multi-word keywords, so called \textit{key phrases}.
By combining words together, the phrases usually gain new meaning that cannot be inferred from the individual words.
Therefore, if we assumed only one-word keywords, we would never find the keyness hidden in the phrases.

The process of identifying keywords is driven by two factors.
First, the more often a word occurs in the document, the more likely it is a keyword.
And second, the more often a word occurs generally in a speech, the less likely it is a keyword of any document.
\cite[ch. 4]{scott2006textual}
The second factor ensures that words we use very often in the speech, such as prepositions, conjuctions or the most common nouns, are not considered as keywords, even though they occur in the document often.

Common implementation of the idea is called the \textit{TF-IDF} algorithm \cite[ch. 6]{manning2008introduction}.
The weight of a term is defined by its \textit{term frequency} and \textit{inverse document frequency}.
The term frequency tf$_{t,d}$ is simply a number of occurrences of the term $t$ in the document $d$. The \textit{inverse document frequency} is defined as
$$
\text{idf}_t = \log{\frac{N}{\text{df}_t}}
$$
where $N$ is the total number of documents and df$_{t}$ is a \textit{document frequency} and is equal to the number of documents, in which the term $t$ occurs.
It is important to take a logarithm of the expression because the \textit{inverted document frequency} can become very large with large collection of documents and we need it to be comparable with the \textit{term frequency}, which is rather small.

The tf-idf weight is computed as follows \cite[ch. 6]{manning2008introduction}:
$$
\text{tf-idf}_{t, d} = \text{tf}_{t,d} \times \text{idf}_t
$$
The \textit{term frequency} ensures higher weight for more frequent terms in the document and \textit{inverse document frequency} ensures higher weight for more obscure words. Now tf-idf$_{t, d}$ can be a good approximation of our keyness measure and is often used as the keyword extraction technique. The last step is to establish a threshold value that distinguishes keywords and non-keywords by comparing it to the tf-idf values of the terms.

\subsection{Named Entity Recognition}
To extract valuable information from the text, we are interested in detecting named entities.
Those are entities that can be referred to with a proper name \cite[ch. 21]{jurafsky2014speech}.
In the speech, they are always noun phrases representing people, places, things, organizations, temporal or numerical expressions, events etc.

Let's pinpoint how important the named entities are in the context of legal cases.
Consider a typical use case of our application: a lawyer is searching through hundreds of emails with different senders and receivers.
The name of the only victim, place and date of the crime are known.
If the lawyer is forced to manually search through the documents, he will filter the emails by occurrences of the aforementioned facts in the text.
All of the facts are named entities, which can be extracted from each document and eventually lead the algorithm to increase the relevance of documents accordingly.
Thanks to the named entities, the algorithm is performing the ranking in the same manner as the lawyer would do, which is our goal.

The academic approach to a find \textit{named entity} is often based on the statistical sequence model \cite[ch. 21]{jurafsky2014speech}.
Entity type and boundaries are found with one pass over the text according to a set of rules.
The rules include \textit{Part-of-Speech Tagging} 
\footnote{Detailed information about POS Tagging can be found in chapter 10 of \citetitle{jurafsky2014speech} by \citeauthor{jurafsky2014speech} \cite{jurafsky2014speech}}
, which helps to find the noun phrases, and therefore, the boundaries.
Next, a \textit{word shape} is examined, which represents a group of words by a general string representation. 
For instance, replacing any letter for 'X' and any digit for 'd', we get a unique word shape 'dd XX' for representing hours of the day written in US format, such as '12 am' or '03 PM'. 
Other rules include prefix matching or dictionary of predefined named entities.

On the other hand, the commercial approach is based on combination of updated dictionaries, rules and supervised machine learning \cite{chiticariu2013rule}.
In the word of Big Data, it has become easier to collect large dictionaries of given names, family names, organizations or geological places (gazetteers).
The named entity recognition is performed by multi pass over the text data.
First, the high-precision but low-recall rules are applied, then more entities are added by substring search of the previous entities.
In the next phase, the entities are consulted with the dictionaries and only then, the statistical sequence model is applied considering all the features from the previous stages \cite[ch. 21]{jurafsky2014speech}.

Further in the thesis and the application, we will work with the following set of \textit{named entities}:
\begin{itemize}
\item Person
\item Organization
\item Location
\item Time
\item Email Address
\item Crime
\item Health Condition
\end{itemize}

\subsection{Relations}
Once we have extracted the \textit{named entities}, we can start looking for the relationships between them.
As soon as we recognize entities and their relations, the main semantic meaning of a sentence is usually fully extracted.
In a legal environment, we can illustrate the added value of a relation by a testimony, in which a witness tells about both a person and a location, however we are interested in the document only if there is a relation between these two, particularly 'was in' or similar.

Let's see how the relations are represented.
Most of the \textit{relations extraction} algorithms are focused on binary relations only.
Multi-entity relations would be very expensive to extract.
We will borrow the representation from the Resource Description Framework (RDF) \cite{lassila1999resource} and define the relation as a triple
$$
\text{<subject, predicate, object>}
$$
where the \textit{subject} is an entity that performs an activity specified in the \textit{predicate} with respect to the \textit{object}.
The \textit{predicate} is the relation for us, it can be for example 'employed by', 'built', 'son of' or 'cheated on'.

The following groups of algorithms can be applied to extract the relations: \textbf{hand-written patterns}, \textbf{supervised machine learning}, \textbf{semi-supervised}, and \textbf{unsupervised}.
We will introduce the first two
\footnote{For more detailed description, see \citetitle{jurafsky2014speech} \cite{jurafsky2014speech}.}.

\subsubsection{\textbf{Hand-written patterns}}
In the work \citetitle{hearst1992automatic} by \citeauthor{hearst1992automatic} \cite{hearst1992automatic} we find the first idea of using patterns to extract relations.
The entity names are ignored and a relation is recognized by a set of patterns.
The patterns are based on the lexicology (POS tagging), the syntax (syntax tree) and the type of the entity (person, location, etc.).
For example, the pattern \cite[sec. 21.2]{jurafsky2014speech}
$$
NP_0\text{ such as }NP_1 \{, NP_2 ..., (\text{and|or})NP_i \}, i \geq 1
$$
is able to catch the relation 'hyponym' for many occurrences, for instance, a sentence
$$
\text{... by any poison, such as Cyanide, Arsenic or Ricin ...}
$$
would generate relations 
\begin{itemize}
\item <poison, hyponym of, Cyanide>
\item <poison, hyponym of, Arsenic>
\item <poison, hyponym of, Ricin>
\end{itemize}
An example of a pattern using entity type \cite[sec. 21.2]{jurafsky2014speech}
$$
\text{\textbf{PERSON} (named|appointed|chose|...) \textbf{PERSON} Prep? \textbf{POSITION}}
$$
will determine a relation 'named'. 

The big disadvantage of the pattern-based recognition is that the patterns need to be written by hand, which takes a lot of work. On the other hand, the precision of such method is usually high since the patterns are directed by a human.

\subsubsection{\textbf{Supervised machine learning}}
The second approach is to annotate a corpus of relations and entities and consider it as a training set in the machine learning.
More precisely, we need to extract features from the sentences, which serve as an input into the ML algorithm and the relations represents the labels for each sentence
\cite[sec 21.2]{jurafsky2014speech}.

The features can consist of unigrams or bigrams of the words in the sentence, especially between the subject and object. Next, the named entity types, POS tags or features from the syntax tree can be included as features. We can also consider the number of words between the subject and object or stemmed version of the words \cite[sec. 21.2]{jurafsky2014speech}.

The algorithm uses two models for the relations extraction.
The first one decides whether there exists a relation between two given entities, and the second one recognizes its type. Then, the algorithm first finds all pairs of entities in the sentence, test it with the first model, and only the positive results are tested on type with the second model \cite[sec. 21.2]{jurafsky2014speech}.

The supervised method can be very accurate on general inputs, however, to be able to do that, a large training dataset is required.
Therefore, the \textit{Distant Supervision for Relation Extraction} can be applied \cite{mintz2009distant}.
It is an elegant way, how to obtain a large training data.
The DBPedia \footnote{Comprehensive database of structured facts extracted from Wikipedia: \mbox{\url{http://wiki.dbpedia.org/}}} or 
Google Knowledge Graph \footnote{Database of world knowledge structured in a graph: \url{https://www.google.com/intl/bn/insidesearch/features/search/knowledge.html}}
provide enough information about some of the relations between real-world objects.
When we search a page about the subject on Wikipedia and find the sentence with the object, then we are ready to adopt the text as an input data for our training phase.
This approach delivers a lot of diverse sentences, in which we are confident about the relation type, and thus the training dataset is rich enough to create a reliable model.

\subsection{Topic Modeling}
\label{topic_modeling}
The \textit{Topic modeling} is a statistical approach for finding general topics that appear in the document.
They keywords are already able to discover some trends of a document, however, unlike the topics, they have to appear in the text.

Prior to the topic recognition, a fixed set of topics have to be defined.
The usual process is to assign probabilities of all topics to every word in the dictionary.
Then, if we consider a document as a bag-of-words
\footnote{Representation of a document which forgets ordering and counts of the words and keep them in a simple set.}, 
we can compute the overall probability that a document has a specific topic only from its words.

One of the well-known algorithms for the topic modeling is called the \textit{Latent Dirichlet allocation} developed by \citeauthor{blei2003latent} in \citeyear{blei2003latent} \cite{blei2003latent}.
The model assumes that every document has fixed distribution of topics it belongs to.
Let's further assume that the words of a document were withdrawn from a specific distribution defined in the following way.
\begin{enumerate}
\item Pick a topic distribution of each document according to a \textit{Dirichlet distribution}.
\item Pick a word distribution for each topic.
\item For all words in all documents, generate the word as follows:
\begin{enumerate}
  \item Pick a random topic according to the distribution of topics for the document.
  \item Pick a random word from the distribution of words for the topic chosen in the step a).
\end{enumerate}
\end{enumerate}
The generation scheme tells us the probability theory for documents and topics, however, it does not tell us how to recognize the topics from the fixed documents.
To apply the theory, we need to reverse the process and assume that the documents were created by this generation process, and eventually guess the topic.
One of the iterative methods, which achieves this goal, is called the \textit{Collapsed Gibbs Sampling} \cite{xiao2010efficient}.
Put simply, the initialization phase picks a random topic for each word in all documents and each next phase goes over every word and changes its topic by computing the probabilities, assuming that all the other words are correctly labeled.



\subsection{Document Category}
There are two types of categories we are interested in.
The first is more related to the structure of the text and tells us where the document is coming from. 
Such types can be for instance email, medical record, police record, affidavit, etc.
The Legato system is prepared to work with such categories, and therefore we rely on the input to contain the information about the first type.

The second type is related to the meaning of the text.
Semantically, it is very similar to the topics introduced in the section \ref{topic_modeling}, and only difference is that we need the most dominant topic to be chosen as a representative.

\section{Introduction of the tools}

\subsection{NLTK: The Natural Language Toolkit}
As a baseline for the commercial NLP tools, we included the open source python framework \textit{NLTK} \cite{bird2004nltk}.
Although it was originally developed for educational purposes, it is now broadly used by students, researchers and public to process the textual data in python.

\textit{NLTK} consists of multiple modules, which can be used as a pipeline during the text processing. 
The basic modules run the tokenizer, POS tagging and syntactic analysis.
More advanced modules are aimed to information extraction and knowledge representation.
The framework comes with large preprocessed corpora that can be used as a reference corpora during the keyword or NE extraction \cite{bird2009natural}.

\subsection{Google Cloud Natural Language}
Google has a long-standing experience with the text processing, especially in the field of information retrieval.
They decided to provide their knowledge of semantic text analysis in a service called Cloud Natural Language \cite{googleNLP}.
Google makes no secret of their approach: the same Deep Learning models that power the Google Search are also employed in the NLP service.

The tool is able to recognize entities, syntactic structure of sentences, category and sentiment  of the text.
The sentiment analysis is recently very interesting topic because companies want to know public opinion on themselves.
However, the sentiment in legal documents is usually neutral and does not play a key role in the relevance classification, therefore we decided not to consider sentiment as a feature.

The \textit{Cloud Natural Language} tool also adds a level of confidence to every entity and category so that the user see how strong each feature is. Moreover, the tool replies with a Wikipedia article for every entity, if exists. Another useful feature is the Translation API, which converts text among many different languages and is able to recognize sources of unknown language. Worth noting that the input does not have to be a pure text but also speech in audio or text in a picture.

\subsection{Watson Natural Language Understanding}
In March 2017, the IBM announced retirement of the \textit{AlchemyAPI} service, which was designed to understand the semantics of text and image by advanced techniques \cite{alchemyRetirement}.
One component of the service was also the \textit{AlchemyLanguage}, providing all NLP features, such as entity, sentiment and topic recognition.
As a replacement, a new service emerged: the \textit{Natural Language Understanding} service \cite{watsonNLP}.

Besides typical NLP features, such as keywords, entities, relations, category, sentiment or topics (which are called concepts in the \textit{NLU}), the service provides emotion recognition (joy, anger, fear, etc.) and metadata recognition.
The latter includes basic information about the author, title, prominent page image, and publication date.
Unfortunately, this feature is available only for HTML pages, which is usually not the type of our documents, hence we will not take advantage of the metadata recognition.

One of the most promising features of the \textit{NLU} service is the model customization.
All the NLP features are always recognized with respect to some general language model, which consists of words from all aspects and domains of our language.
This might be a disadvantage, especially when our documents are actually always of one type and contains words from specific domain.
For instance, the general model would always consider 'law', 'justice' or ' sentence' as keywords in our legal documents, however, they might appear in most of the documents and become not relevant anymore in the legal domain.
The \textit{NLU} service enables a user to create its own language model and load it into the cloud.
More than one model can be active and the user can switch between them before each query.

\subsection{Aylien}
\textit{Aylien} is a software package of information retrieval, machine learning, and natural language processing \cite{aylienNLP}, which provides competitive features to the mentioned NLP services from Google and IBM.

In addition to the classical NLP features, \textit{Aylien} provides summarization, which essentially picks the most important sentences from the text. In fact, this feature makes a good sense in our legal domain. Next innovative feature looks for related phrases, which are semantically as close to the original phrase as possible. Another advantage of \textit{Aylien} is that it includes time-based entities in the named entity recognition, specifically dates. The previously mentioned services surprisingly ignore the time and date expressions most of the time.

Next interesting feature of \textit{Aylien} is the hashtag suggestion, however, it is basically the topic modeling as we described it in \ref{topic_modeling}.
In the same manner, the article extraction is another term for the IBM's metadata recognition.
Truly new feature is the aspect-based sentiment analysis, in which the sentiment is recognized for each aspect, not generally for the whole text.
For example, an analysis of hotel reviews can discover angry opinions on Wi-Fi signal but excellent rating of the staff.

\section{Comparison}
To compare the tools, we had to restrict the evaluation set only to the common features included in all the tools.
Specifically, the \textit{keywords} and \textit{named entities}: persons, organizations, and locations.
Without any comparison, we also evaluated the relations, which were supported only by \textit{Watson NLU}, and dates supported only by \textit{Aylien}.

\subsection{Testing data: Legal mock case}
Of course, we could not use any real-world case for our testing and analysis due to privacy reasons, therefore, we analyzed a legal mock case called \textit{Davis v. HappyLand Toy Company}\cite{american2015davis} created by the \textit{American Mock Trial Association}\footnote{http://www.collegemocktrial.org/}. 
The association creates legal cases for educational purposes, which then serve as a material for the mock trial competitions.

Briefly described, the case puts a father of a child, who was killed by swallowed part of a toy, against the company that produced the dangerous toy. For our use case, we picked five most interesting and distinct documents: 
\begin{enumerate}
\item An email of an inter-company communication about the substances used in the toy.
\item Affidavit (testimony) of the father.
\item Affidavit of the baby sitter, who looked after the child at the moment of the fatality.
\item Medical record from the autopsy of the child.
\item Journal paper about the origin, behavior, look and impacts on a human body of the substance used in the toy.
\end{enumerate}

\subsection{Survey}
The survey was created in a way that no knowledge of law is necessary, the questions are simple and clearly stated.
Some of the documents are several pages long, hence the survey can be saved and finished later.
The respondents were not provided with any extra information about the legal case, so that the results are comparable with the NLP tools, which have no information as well.

We asked the same set of questions for every document.
First two questions asked about the keywords.
We have split the one-word phrases and multi-word phrases and required both with at least 10 one-word keywords.
Next, the respondent has to type people, organizations and locations that appear in the text.
Moreover, we require relationships between people and dates together with a label specifying which event is related to the date.
Next two questions were related to the semantics and were not used to evaluate the tools.
We asked the respondents to copy short passage from the text that could be used as an evidence at the court.
The second question asks for a short summary of the whole document.
Both of the questions will be later applied in the training and evaluation of the models
for document relevancy.

\subsection{Approach in NLTK}
[mozna presunout do NLTK intro, sem se to moc nehodi]

Keywords
- filtering on Brown corpus
- only nouns
- reference corpus Reuters
- tf-idf

Entities
- look at the doc, their apporach

\subsection{Statistical comparison}
[jackard coefficient do tabulky]

[union of results from respondents, then kappa measure - neouzitelna, rict proc, pouzil jsem jackard coefficient]

\subsection{Analytical comparison}
[vypichnout par spatnych a dobrych prikladu a popsat v cem se tooly lisi]


\chapter{Classification}
\section{Attributes}
\section{Models}

\chapter{Implementation}
\section{Frameworks and libraries}
\section{Design and implementation}
\section{Integration into Legato}

\chapter{Evaluation}
\section{Dataset introduction}
\section{Model selection}
\section{Parameter tuning}
\section{Results}

\chapter{Conclusion}



  \printbibliography[heading=bibintoc] %% Print the bibliography.

  \makeatletter\thesis@blocks@clear\makeatother
  \phantomsection %% Print the index and insert it into the
  \addcontentsline{toc}{chapter}{\indexname} %% table of contents.
  \printindex

\appendix %% Start the appendices.
\chapter{An appendix}
Here you can insert the appendices of your thesis.

\end{document}
